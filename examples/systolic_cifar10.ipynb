{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18ebda70",
   "metadata": {},
   "source": [
    "# # End-to-End: Build → Calibrate → (Re)Train → Compare Accuracy (16×16) → Compare Runtime (various SA)\n",
    "\n",
    "This notebook follows the same pattern as before, but bundles the whole workflow:\n",
    "\n",
    "1. **Build models** for two multipliers (`mul8s_acc` and `mul8s_1L2H`) and a fixed SA size (16×16).\n",
    "2. **Calibrate** both models (percentile histogram, short pass).\n",
    "3. **(Optional) Re-train** both models briefly (fine-tune) to reduce quantization/approx errors.\n",
    "4. **Compare accuracy** between the two 16×16 models (calibrated or fine-tuned).\n",
    "5. **Compare execution time** for a chosen multiplier across multiple SA sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f70980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, timeit\n",
    "import torch\n",
    "import torchvision as tv\n",
    "from torchvision import transforms as T\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Safer dataloaders in constrained environments\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
    "\n",
    "# ---- Global config ----\n",
    "DEVICE = \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "NUM_CALIB_BATCHES = 2        # increase to 8–32 for better INT8 quality\n",
    "SA_ROWS, SA_COLS = 16, 16    # fixed for the accuracy comparison\n",
    "USE_EXACT = False            # True -> force exact mult; False -> use approx variants\n",
    "\n",
    "# Two variants to compare (16×16 accuracy)\n",
    "VARIANTS = [\"mul8s_acc\", \"mul8s_1L2H\"]\n",
    "\n",
    "# (Optional) training settings\n",
    "DO_FINETUNE = False          # set True to run a brief fine-tune \n",
    "FINETUNE_EPOCHS = 1\n",
    "LR = 1e-4\n",
    "WD = 0.0\n",
    "\n",
    "# SA configs for runtime comparison \n",
    "SA_CONFIGS = [(8,8), (16,16), (32,8), (8,32)]\n",
    "RUNTIME_MULT = \"mul8s_acc\"   # which multiplier to use for the runtime sweep\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 160)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fddaeb6",
   "metadata": {},
   "source": [
    "## Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa60aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_dataloader(mean = (0.4914, 0.4822, 0.4465), std = (0.2471, 0.2435, 0.2616)):\n",
    "\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean, std),\n",
    "        ]\n",
    "    )\n",
    "    dataset = CIFAR10(root=\"datasets/cifar10_data\", train=False, download=True, transform=transform)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=True,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "transform = T.Compose(\n",
    "        [\n",
    "            T.RandomCrop(32, padding=4),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean = (0.4914, 0.4822, 0.4465), std = (0.2471, 0.2435, 0.2616)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "dataset = CIFAR10(root=\"datasets/cifar10_data\", train=True, download=True, transform=transform)\n",
    "\n",
    "evens = list(range(0, len(dataset), 10))\n",
    "trainset_1 = torch.utils.data.Subset(dataset, evens)\n",
    "\n",
    "data = val_dataloader()\n",
    "\n",
    "# data_t is used for calibration purposes and is a subset of train-set\n",
    "data_t = DataLoader(trainset_1, batch_size=128,\n",
    "                                            shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e637253",
   "metadata": {},
   "source": [
    "## Helpers (evaluate, calibration, amax, finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc6531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_quantization.nn.modules.tensor_quantizer import TensorQuantizer\n",
    "from pytorch_quantization import calib\n",
    "from adapt.approx_layers.systolic_build import precompile_systolic_extensions\n",
    "from adapt.approx_layers.systolic_utils import swap_to_systolic\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import contextlib\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device=DEVICE, desc=\"Eval\"):\n",
    "    model.eval(); model.to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    model.eval()\n",
    "    start_time = timeit.default_timer()\n",
    "    with torch.no_grad():\n",
    "        for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "            images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(timeit.default_timer() - start_time)\n",
    "    print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))\n",
    "    return 100 * correct/total\n",
    "\n",
    "@torch.no_grad()\n",
    "def init_weight_amax_from_weights(model):\n",
    "    n_set, n_total = 0, 0\n",
    "    for _, m in model.named_modules():\n",
    "        q_w = getattr(m, \"quantizer_w\", None)\n",
    "        if isinstance(q_w, quant_nn.TensorQuantizer):\n",
    "            n_total += 1\n",
    "            if getattr(q_w, \"amax\", None) is None:\n",
    "                W = getattr(m, \"weight\", None)\n",
    "                if W is not None:\n",
    "                    q_w._amax = torch.as_tensor(W.detach().abs().max(), dtype=torch.float32)\n",
    "                    n_set += 1\n",
    "    print(f\"[init_weight_amax_from_weights] set {n_set}/{n_total} weight amax\")\n",
    "\n",
    "# 2) A small helper to attach pre-hooks that \"touch\" quantizers during calib\n",
    "def _make_calib_pre_hook(mod):\n",
    "    @torch.no_grad()\n",
    "    def _pre(mod_, inputs):\n",
    "        # inputs is a tuple; x is first positional input tensor\n",
    "        if not inputs:\n",
    "            return\n",
    "        x = inputs[0]\n",
    "        # Call activation quantizer to collect stats\n",
    "        q = getattr(mod_, \"quantizer\", None)\n",
    "        if isinstance(q, quant_nn.TensorQuantizer) and getattr(q, \"_calibrator\", None) is not None:\n",
    "            q(x)\n",
    "        # Touch weights as well so weight calibrator (if any) can record\n",
    "        q_w = getattr(mod_, \"quantizer_w\", None)\n",
    "        if isinstance(q_w, quant_nn.TensorQuantizer) and getattr(q_w, \"_calibrator\", None) is not None:\n",
    "            W = getattr(mod_, \"weight\", None)\n",
    "            if W is not None:\n",
    "                q_w(W)\n",
    "    return _pre\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def attach_calibration_hooks(model):\n",
    "    \"\"\"Attach forward-pre-hooks to every systolic layer so quantizers see tensors.\"\"\"\n",
    "    hooks = []\n",
    "    for _, m in model.named_modules():\n",
    "        # We only hook layers that have our quantizers AND a forward\n",
    "        has_any_q = isinstance(getattr(m, \"quantizer\", None), quant_nn.TensorQuantizer) or \\\n",
    "                    isinstance(getattr(m, \"quantizer_w\", None), quant_nn.TensorQuantizer)\n",
    "        if has_any_q:\n",
    "            try:\n",
    "                h = m.register_forward_pre_hook(_make_calib_pre_hook(m))\n",
    "                hooks.append(h)\n",
    "            except Exception:\n",
    "                pass\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        for h in hooks:\n",
    "            try:\n",
    "                h.remove()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "# 3) Fixed collect_stats using the hooks\n",
    "def collect_stats(model, data_loader, num_batches=10, device=\"cpu\"):\n",
    "    \"\"\"Collect activation histograms. Uses pre-hooks to ensure quantizers see tensors.\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Enable calibration (disable quantization) so quantizers record histograms\n",
    "    for _, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                module.disable_quant()\n",
    "                module.enable_calib()\n",
    "            else:\n",
    "                module.disable()\n",
    "\n",
    "    with torch.no_grad(), attach_calibration_hooks(model):\n",
    "        for i, (image, _) in enumerate(data_loader):\n",
    "            image = image.to(device, non_blocking=True)\n",
    "            _ = model(image)  # pre-hooks will call quantizers here\n",
    "            if i >= num_batches - 1:\n",
    "                break\n",
    "\n",
    "    # Disable calibration (enable quantization for inference)\n",
    "    for _, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                module.enable_quant()\n",
    "                module.disable_calib()\n",
    "            else:\n",
    "                module.enable()\n",
    "\n",
    "    print(\"Calibration data collection complete.\")\n",
    "\n",
    "# 4) Compute and sanitize amax \n",
    "def compute_amax(model, method=\"percentile\", percentile=99.99, strict=False, fallback=1.0):\n",
    "    n_loaded, n_fixed = 0, 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                try:\n",
    "                    if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "                        module.load_calib_amax(strict=strict)\n",
    "                    else:\n",
    "                        module.load_calib_amax(method=method, percentile=percentile, strict=strict)\n",
    "                    n_loaded += 1\n",
    "                except RuntimeError:\n",
    "                    pass\n",
    "            # sanitize\n",
    "            amax = getattr(module, \"amax\", None)\n",
    "            if (amax is None) or (torch.isnan(amax)) or (float(amax) == 0.0):\n",
    "                module._amax = torch.tensor(float(fallback), dtype=torch.float32)\n",
    "                n_fixed += 1\n",
    "            print(F\"{name:40}: {module}\")\n",
    "    print(f\"Loaded calibrated amax values. loaded={n_loaded}, sanitized={n_fixed}\")\n",
    "    model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d74a7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import calib\n",
    "from adapt.references.classification.train import train_one_epoch, load_data\n",
    "def finetune_one_epoch(model, loader, device=DEVICE, lr=1e-4, wd=0.0):\n",
    "    from pytorch_quantization import nn as quant_nn\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "    \n",
    "    # finetune the model for one epoch based on data_t subset \n",
    "    train_one_epoch(model, criterion, optimizer, loader, device, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5956b04c",
   "metadata": {},
   "source": [
    "## Accuracy comparison ( systolic array size 16×16) for two multipliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3eca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Accuracy comparison @16x16 using resnet50_systolic ===\n",
    "rows = []\n",
    "\n",
    "try:\n",
    "    from models.resnet_systolic import resnet50_systolic\n",
    "except Exception:\n",
    "    from examples.models.resnet_systolic import resnet50_systolic\n",
    "\n",
    "for axx_mult in VARIANTS:  # z.B. [\"mul8s_acc\", \"mul8s_1L2H\"]\n",
    "    print(f\"=== resnet50_systolic @16x16: {axx_mult} ===\")\n",
    "\n",
    "    # 1) Precompile \n",
    "    precompile_systolic_extensions(\n",
    "        axx_mult=axx_mult,\n",
    "        use_exact_variants=(USE_EXACT,),\n",
    "        sa_rows=16, sa_cols=16,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    model = resnet50_systolic(pretrained=True, axx_mult=axx_mult, use_exact=USE_EXACT)\n",
    "    model.eval()  \n",
    "\n",
    "    # 3) calibration\n",
    "    with torch.no_grad():\n",
    "        #init_weight_amax_from_weights(model)  \n",
    "        stats = collect_stats(model, data_t, num_batches=2)\n",
    "        amax = compute_amax(model, method=\"percentile\", percentile=99.99)\n",
    "    \n",
    "    # optional - test different calibration methods\n",
    "    #amax = compute_amax(model, method=\"mse\")\n",
    "    #amax = compute_amax(model, method=\"entropy\")\n",
    "\n",
    "\n",
    "    acc_cal = evaluate(model, data, DEVICE, desc=f\"Eval resnet50_systolic ({axx_mult}) calibrated\")\n",
    "    rows.append({\"Variant\": axx_mult, \"Type\": \"calibrated\", \"Accuracy %\": acc_cal})\n",
    "\n",
    "    import torch.nn as nn\n",
    "\n",
    "    if DO_FINETUNE:\n",
    "        \n",
    "        for epoch in range(FINETUNE_EPOCHS):\n",
    "            finetune_one_epoch(model, data_t, lr=LR, wd=WD)\n",
    "        model.eval()\n",
    "        acc_ft = evaluate(model, data, 'cpu',desc=f\"Eval resnet50_systolic ({axx_mult}) finetuned\")\n",
    "        rows.append({\"Variant\": axx_mult, \"Type\": \"finetuned\", \"Accuracy %\": acc_ft})\n",
    "\n",
    "df_acc = pd.DataFrame(rows).sort_values(by=[\"Variant\", \"Type\"]).reset_index(drop=True)\n",
    "df_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f9d5ed",
   "metadata": {},
   "source": [
    "## Runtime comparison across SA sizes (same multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30d2e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compare elapsed time for a short pass (10 batches) and full evaluation.\n",
    "results = []\n",
    "\n",
    "for (r, c) in SA_CONFIGS:\n",
    "    print(f\"=== Runtime: {RUNTIME_MULT} @ SA {r}x{c} ===\")\n",
    "    precompile_systolic_extensions(axx_mult=RUNTIME_MULT, use_exact_variants=(USE_EXACT,),\n",
    "                                   sa_rows=r, sa_cols=c, verbose=False)\n",
    "    \n",
    "    model = resnet50_systolic(pretrained=True, axx_mult=axx_mult, use_exact=USE_EXACT,sa_rows=r, sa_cols=c)\n",
    "    model.eval() \n",
    "\n",
    "    # Calibrate quickly (reuse same approach for fair comparison)\n",
    "    with torch.no_grad():\n",
    "        stats = collect_stats(model, data_t, num_batches=2)\n",
    "        amax = compute_amax(model, method=\"percentile\", percentile=99.99)    \n",
    "\n",
    "    # Warmup\n",
    "    xb, yb = next(iter(data))\n",
    "    _ = model(xb)\n",
    "\n",
    "    # Short run (10 batches)\n",
    "    iters = 10\n",
    "    start = timeit.default_timer()\n",
    "    with torch.no_grad():\n",
    "        for i, (x, _) in enumerate(data):\n",
    "            _ = model(x)\n",
    "            if i >= iters - 1:\n",
    "                break\n",
    "    t_small = timeit.default_timer() - start\n",
    "\n",
    "    # Full eval timing\n",
    "    start = timeit.default_timer()\n",
    "    acc = evaluate(model, data, desc=f\"Eval runtime SA {r}x{c}\")\n",
    "    t_full = timeit.default_timer() - start\n",
    "\n",
    "    results.append({\n",
    "        \"sa_rows\": r,\n",
    "        \"sa_cols\": c,\n",
    "        \"accuracy %\": acc,\n",
    "        \"time_10_batches_sec\": t_small,\n",
    "        \"time_full_eval_sec\": t_full,\n",
    "    })\n",
    "\n",
    "df_rt = pd.DataFrame(results).sort_values(by=[\"sa_rows\",\"sa_cols\"]).reset_index(drop=True)\n",
    "df_rt\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
