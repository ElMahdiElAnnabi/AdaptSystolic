{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18ebda70",
   "metadata": {},
   "source": [
    "# # End-to-End: Build → Calibrate → (Re)Train → Compare Accuracy (16×16) → Compare Runtime (various SA)\n",
    "\n",
    "This notebook follows the same pattern as before, but bundles the whole workflow:\n",
    "\n",
    "1. **Build models** for two multipliers (`mul8s_acc` and `mul8s_1L2H`) and a fixed SA size (16×16).\n",
    "2. **Calibrate** both models (percentile histogram, short pass).\n",
    "3. **(Optional) Re-train** both models briefly (fine-tune) to reduce quantization/approx errors.\n",
    "4. **Compare accuracy** between the two 16×16 models (calibrated or fine-tuned).\n",
    "5. **Compare execution time** for a chosen multiplier across multiple SA sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8f70980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, timeit\n",
    "import torch\n",
    "import torchvision as tv\n",
    "from torchvision import transforms as T\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Safer dataloaders in constrained environments\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
    "\n",
    "# ---- Global config ----\n",
    "DEVICE = \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "NUM_CALIB_BATCHES = 2        # increase to 8–32 for better INT8 quality\n",
    "SA_ROWS, SA_COLS = 16, 16    # fixed for the accuracy comparison\n",
    "USE_EXACT = False            # True -> force exact mult; False -> use approx variants\n",
    "\n",
    "# Two variants to compare (16×16 accuracy)\n",
    "VARIANTS = [\"mul8s_acc\", \"mul8s_1L2H\"]\n",
    "\n",
    "# (Optional) training settings\n",
    "DO_FINETUNE = False          # set True to run a brief fine-tune \n",
    "FINETUNE_EPOCHS = 1\n",
    "LR = 1e-4\n",
    "WD = 0.0\n",
    "\n",
    "# SA configs for runtime comparison \n",
    "SA_CONFIGS = [(8,8), (16,16), (32,8), (8,32)]\n",
    "RUNTIME_MULT = \"mul8s_acc\"   # which multiplier to use for the runtime sweep\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 160)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fddaeb6",
   "metadata": {},
   "source": [
    "## Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aa60aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def val_dataloader(mean = (0.4914, 0.4822, 0.4465), std = (0.2471, 0.2435, 0.2616)):\n",
    "\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean, std),\n",
    "        ]\n",
    "    )\n",
    "    dataset = CIFAR10(root=\"datasets/cifar10_data\", train=False, download=True, transform=transform)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=True,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "transform = T.Compose(\n",
    "        [\n",
    "            T.RandomCrop(32, padding=4),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean = (0.4914, 0.4822, 0.4465), std = (0.2471, 0.2435, 0.2616)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "dataset = CIFAR10(root=\"datasets/cifar10_data\", train=True, download=True, transform=transform)\n",
    "\n",
    "evens = list(range(0, len(dataset), 10))\n",
    "trainset_1 = torch.utils.data.Subset(dataset, evens)\n",
    "\n",
    "data = val_dataloader()\n",
    "\n",
    "# data_t is used for calibration purposes and is a subset of train-set\n",
    "data_t = DataLoader(trainset_1, batch_size=128,\n",
    "                                            shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e637253",
   "metadata": {},
   "source": [
    "## Helpers (evaluate, calibration, amax, finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fc6531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_quantization.nn.modules.tensor_quantizer import TensorQuantizer\n",
    "from pytorch_quantization import calib\n",
    "from adapt.approx_layers.systolic_build import precompile_systolic_extensions\n",
    "from adapt.approx_layers.systolic_utils import swap_to_systolic\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import contextlib\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device=DEVICE, desc=\"Eval\"):\n",
    "    model.eval(); model.to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    model.eval()\n",
    "    start_time = timeit.default_timer()\n",
    "    with torch.no_grad():\n",
    "        for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "            images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(timeit.default_timer() - start_time)\n",
    "    print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))\n",
    "    return 100 * correct/total\n",
    "\n",
    "@torch.no_grad()\n",
    "def init_weight_amax_from_weights(model):\n",
    "    n_set, n_total = 0, 0\n",
    "    for _, m in model.named_modules():\n",
    "        q_w = getattr(m, \"quantizer_w\", None)\n",
    "        if isinstance(q_w, quant_nn.TensorQuantizer):\n",
    "            n_total += 1\n",
    "            if getattr(q_w, \"amax\", None) is None:\n",
    "                W = getattr(m, \"weight\", None)\n",
    "                if W is not None:\n",
    "                    q_w._amax = torch.as_tensor(W.detach().abs().max(), dtype=torch.float32)\n",
    "                    n_set += 1\n",
    "    print(f\"[init_weight_amax_from_weights] set {n_set}/{n_total} weight amax\")\n",
    "\n",
    "# 2) A small helper to attach pre-hooks that \"touch\" quantizers during calib\n",
    "def _make_calib_pre_hook(mod):\n",
    "    @torch.no_grad()\n",
    "    def _pre(mod_, inputs):\n",
    "        # inputs is a tuple; x is first positional input tensor\n",
    "        if not inputs:\n",
    "            return\n",
    "        x = inputs[0]\n",
    "        # Call activation quantizer to collect stats\n",
    "        q = getattr(mod_, \"quantizer\", None)\n",
    "        if isinstance(q, quant_nn.TensorQuantizer) and getattr(q, \"_calibrator\", None) is not None:\n",
    "            q(x)\n",
    "        # Touch weights as well so weight calibrator (if any) can record\n",
    "        q_w = getattr(mod_, \"quantizer_w\", None)\n",
    "        if isinstance(q_w, quant_nn.TensorQuantizer) and getattr(q_w, \"_calibrator\", None) is not None:\n",
    "            W = getattr(mod_, \"weight\", None)\n",
    "            if W is not None:\n",
    "                q_w(W)\n",
    "    return _pre\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def attach_calibration_hooks(model):\n",
    "    \"\"\"Attach forward-pre-hooks to every systolic layer so quantizers see tensors.\"\"\"\n",
    "    hooks = []\n",
    "    for _, m in model.named_modules():\n",
    "        # We only hook layers that have our quantizers AND a forward\n",
    "        has_any_q = isinstance(getattr(m, \"quantizer\", None), quant_nn.TensorQuantizer) or \\\n",
    "                    isinstance(getattr(m, \"quantizer_w\", None), quant_nn.TensorQuantizer)\n",
    "        if has_any_q:\n",
    "            try:\n",
    "                h = m.register_forward_pre_hook(_make_calib_pre_hook(m))\n",
    "                hooks.append(h)\n",
    "            except Exception:\n",
    "                pass\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        for h in hooks:\n",
    "            try:\n",
    "                h.remove()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "# 3) Fixed collect_stats using the hooks\n",
    "def collect_stats(model, data_loader, num_batches=10, device=\"cpu\"):\n",
    "    \"\"\"Collect activation histograms. Uses pre-hooks to ensure quantizers see tensors.\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Enable calibration (disable quantization) so quantizers record histograms\n",
    "    for _, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                module.disable_quant()\n",
    "                module.enable_calib()\n",
    "            else:\n",
    "                module.disable()\n",
    "\n",
    "    with torch.no_grad(), attach_calibration_hooks(model):\n",
    "        for i, (image, _) in enumerate(data_loader):\n",
    "            image = image.to(device, non_blocking=True)\n",
    "            _ = model(image)  # pre-hooks will call quantizers here\n",
    "            if i >= num_batches - 1:\n",
    "                break\n",
    "\n",
    "    # Disable calibration (enable quantization for inference)\n",
    "    for _, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                module.enable_quant()\n",
    "                module.disable_calib()\n",
    "            else:\n",
    "                module.enable()\n",
    "\n",
    "    print(\"Calibration data collection complete.\")\n",
    "\n",
    "# 4) Compute and sanitize amax \n",
    "def compute_amax(model, method=\"percentile\", percentile=99.99, strict=False, fallback=1.0):\n",
    "    n_loaded, n_fixed = 0, 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                try:\n",
    "                    if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "                        module.load_calib_amax(strict=strict)\n",
    "                    else:\n",
    "                        module.load_calib_amax(method=method, percentile=percentile, strict=strict)\n",
    "                    n_loaded += 1\n",
    "                except RuntimeError:\n",
    "                    pass\n",
    "            # sanitize\n",
    "            amax = getattr(module, \"amax\", None)\n",
    "            if (amax is None) or (torch.isnan(amax)) or (float(amax) == 0.0):\n",
    "                module._amax = torch.tensor(float(fallback), dtype=torch.float32)\n",
    "                n_fixed += 1\n",
    "            print(F\"{name:40}: {module}\")\n",
    "    print(f\"Loaded calibrated amax values. loaded={n_loaded}, sanitized={n_fixed}\")\n",
    "    model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d74a7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import calib\n",
    "from adapt.references.classification.train import train_one_epoch, load_data\n",
    "def finetune_one_epoch(model, loader, device=DEVICE, lr=1e-4, wd=0.0):\n",
    "    from pytorch_quantization import nn as quant_nn\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "    \n",
    "    # finetune the model for one epoch based on data_t subset \n",
    "    train_one_epoch(model, criterion, optimizer, loader, device, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5956b04c",
   "metadata": {},
   "source": [
    "## Accuracy comparison ( systolic array size 16×16) for two multipliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae3eca79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== resnet50_systolic @16x16: mul8s_acc ===\n",
      "Pre-compiling systolic extensions for mul8s_acc...\n",
      "  Mode: approx\n",
      "    • linear (r16×c16)\n",
      "    • conv2d (r16×c16)\n",
      "Pre-compilation complete! Models will now load instantly.\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic linear kernel: mul8s_acc, exact=False, SA=16x16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1111 10:17:08.398038 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.398667 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.399150 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.399617 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.400077 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.400535 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.401030 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.401548 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.402094 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.402627 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.404590 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.405132 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.407149 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.407780 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.408473 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.409189 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.409868 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.410512 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.411228 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.411933 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.413017 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.413555 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.414067 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.414623 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.415177 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.415678 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.416147 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.416624 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.417132 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.417574 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.418007 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.418596 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.419209 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.419755 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.420299 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.420957 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.421550 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.422088 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.422859 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.423965 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.425017 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.426021 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.426947 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.427738 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.428439 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.429140 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.429803 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.430538 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.431298 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.431973 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.432611 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.433287 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.433956 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.434556 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.435109 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.435648 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.436161 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.436643 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.437168 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.437741 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.438420 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.439166 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.440128 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.441089 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.441855 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.442750 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.443632 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.444555 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.445648 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.446599 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.447364 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.448068 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.448788 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.449451 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.450129 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.450725 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.451265 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.451762 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.452250 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.452782 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.453379 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.454196 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.455151 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.456506 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.458193 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.459616 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.460484 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.461162 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.461686 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.462180 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1111 10:17:08.462644 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.463077 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.463559 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.464105 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.464596 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.465112 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.465624 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.466152 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.466735 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.467173 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.467830 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.468457 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.469076 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.469661 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.474270 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.481100 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.481610 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.482054 140134759839552 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1111 10:17:08.483283 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.483695 140134759839552 tensor_quantizer.py:238] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "W1111 10:17:08.484882 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.486027 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.487235 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.488598 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.490059 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.491409 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.492867 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.494054 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.495213 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.496431 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.497630 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.498825 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.499969 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.500968 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.501890 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.502809 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.503735 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.504646 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.505558 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.506443 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.507273 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.508070 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.508795 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.509526 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.510290 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.511106 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.511962 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.512823 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.513643 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.514437 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.523265 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.531184 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.532490 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.533781 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.535113 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.536416 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.537828 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.538868 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.540337 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.541975 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.543412 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.545168 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.546400 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.547710 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.548926 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.550100 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.551181 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.552289 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.553327 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.554309 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.555312 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.556661 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.558518 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.560839 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.562795 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.564728 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.566196 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.567698 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.569195 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.570445 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1111 10:17:08.571635 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.573474 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.575646 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.579190 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.581215 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.583300 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.584754 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.586052 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.587328 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.588169 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.589820 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.591092 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.592367 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.593663 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.594941 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.596336 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.597548 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.598705 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.600013 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.601653 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.603249 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.604777 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.606297 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.608187 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.609181 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.609978 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.611668 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.612920 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.614113 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.615278 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.616600 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.617735 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.618770 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.619804 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.620843 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.621811 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.622804 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.623929 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.624958 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.625953 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.627128 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.628160 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.629091 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.630049 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.630957 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.632037 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1111 10:17:08.633294 140134759839552 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration data collection complete.\n",
      "conv1.quantizer                         : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator quant)\n",
      "conv1.quantizer_w                       : TensorQuantizer(8bit per-tensor amax=0.1625 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.6109 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0804 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2126 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0355 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2418 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0544 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.6109 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0867 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2267 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0270 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1351 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0287 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1629 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0338 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2277 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0412 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1761 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0334 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3360 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0479 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2311 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0423 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1700 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0212 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1988 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0298 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.2311 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0273 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1450 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0200 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1393 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0167 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1565 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0267 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1443 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0243 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1422 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0189 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1594 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0303 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1448 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0273 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1488 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0215 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1715 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0277 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1583 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0266 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1382 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0128 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1936 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0160 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.1583 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0104 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0990 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0125 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0937 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0107 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0894 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0177 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1072 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0122 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0823 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0875 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0172 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1203 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0111 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0958 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0081 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1270 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0151 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1361 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0125 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1060 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0084 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1359 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0161 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1896 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0124 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1139 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0068 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1457 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0185 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4986 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0093 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0831 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0029 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2695 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0087 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4986 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0131 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5338 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0029 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1440 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0040 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3068 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0107 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.8076 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0024 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1300 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0042 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3065 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0111 calibrator=HistogramCalibrator quant)\n",
      "fc.quantizer                            : TensorQuantizer(8bit per-tensor amax=1.0638 calibrator=HistogramCalibrator quant)\n",
      "fc.quantizer_w                          : TensorQuantizer(8bit per-tensor amax=0.1751 calibrator=HistogramCalibrator quant)\n",
      "Loaded calibrated amax values. loaded=108, sanitized=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████████████████████████████████████▉ | 76/78 [41:09<01:04, 32.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     amax \u001b[38;5;241m=\u001b[39m compute_amax(model, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpercentile\u001b[39m\u001b[38;5;124m\"\u001b[39m, percentile\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m99.99\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# optional - test different calibration methods\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#amax = compute_amax(model, method=\"mse\")\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#amax = compute_amax(model, method=\"entropy\")\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m acc_cal \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEval resnet50_systolic (\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43maxx_mult\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m) calibrated\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m rows\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariant\u001b[39m\u001b[38;5;124m\"\u001b[39m: axx_mult, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibrated\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m: acc_cal})\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, loader, device, desc)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteraction, (images, labels) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(data), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(data)):\n\u001b[1;32m     19\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m), labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     22\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m/workspace/adapt/examples/models/resnet_systolic.py:232\u001b[0m, in \u001b[0;36mResNetSystolic.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    231\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x))))\n\u001b[0;32m--> 232\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    233\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[1;32m    234\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py:119\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m/workspace/adapt/examples/models/resnet_systolic.py:128\u001b[0m, in \u001b[0;36mBottleneckSystolic.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    126\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))\n\u001b[1;32m    127\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out)))\n\u001b[0;32m--> 128\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     identity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m/workspace/adapt/adapt/approx_layers/layers_systolic.py:179\u001b[0m, in \u001b[0;36mAdaPT_Conv2d_Systolic.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    177\u001b[0m     out_i32 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(outs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     out_i32 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxx_conv2d_kernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m scale \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_value \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantizer\u001b[38;5;241m.\u001b[39mamax) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_value \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantizer_w\u001b[38;5;241m.\u001b[39mamax)\n\u001b[1;32m    182\u001b[0m out \u001b[38;5;241m=\u001b[39m out_i32\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m scale\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# === Accuracy comparison @16x16 using resnet50_systolic ===\n",
    "rows = []\n",
    "\n",
    "try:\n",
    "    from models.resnet_systolic import resnet50_systolic\n",
    "except Exception:\n",
    "    from examples.models.resnet_systolic import resnet50_systolic\n",
    "\n",
    "for axx_mult in VARIANTS:  # z.B. [\"mul8s_acc\", \"mul8s_1L2H\"]\n",
    "    print(f\"=== resnet50_systolic @16x16: {axx_mult} ===\")\n",
    "\n",
    "    # 1) Precompile \n",
    "    precompile_systolic_extensions(\n",
    "        axx_mult=axx_mult,\n",
    "        use_exact_variants=(USE_EXACT,),\n",
    "        sa_rows=16, sa_cols=16,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    model = resnet50_systolic(pretrained=True, axx_mult=axx_mult, use_exact=USE_EXACT)\n",
    "    model.eval()  \n",
    "\n",
    "    # 3) calibration\n",
    "    with torch.no_grad():\n",
    "        #init_weight_amax_from_weights(model)  \n",
    "        stats = collect_stats(model, data_t, num_batches=2)\n",
    "        amax = compute_amax(model, method=\"percentile\", percentile=99.99)\n",
    "    \n",
    "    # optional - test different calibration methods\n",
    "    #amax = compute_amax(model, method=\"mse\")\n",
    "    #amax = compute_amax(model, method=\"entropy\")\n",
    "\n",
    "\n",
    "    acc_cal = evaluate(model, data, DEVICE, desc=f\"Eval resnet50_systolic ({axx_mult}) calibrated\")\n",
    "    rows.append({\"Variant\": axx_mult, \"Type\": \"calibrated\", \"Accuracy %\": acc_cal})\n",
    "\n",
    "    import torch.nn as nn\n",
    "\n",
    "    if DO_FINETUNE:\n",
    "        \n",
    "        for epoch in range(FINETUNE_EPOCHS):\n",
    "            finetune_one_epoch(model, data_t, lr=LR, wd=WD)\n",
    "        model.eval()\n",
    "        acc_ft = evaluate(model, data, 'cpu',desc=f\"Eval resnet50_systolic ({axx_mult}) finetuned\")\n",
    "        rows.append({\"Variant\": axx_mult, \"Type\": \"finetuned\", \"Accuracy %\": acc_ft})\n",
    "\n",
    "df_acc = pd.DataFrame(rows).sort_values(by=[\"Variant\", \"Type\"]).reset_index(drop=True)\n",
    "df_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f9d5ed",
   "metadata": {},
   "source": [
    "## Runtime comparison across SA sizes (same multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30d2e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compare elapsed time for a short pass (10 batches) and full evaluation.\n",
    "results = []\n",
    "\n",
    "for (r, c) in SA_CONFIGS:\n",
    "    print(f\"=== Runtime: {RUNTIME_MULT} @ SA {r}x{c} ===\")\n",
    "    precompile_systolic_extensions(axx_mult=RUNTIME_MULT, use_exact_variants=(USE_EXACT,),\n",
    "                                   sa_rows=r, sa_cols=c, verbose=False)\n",
    "    \n",
    "    model = resnet50_systolic(pretrained=True, axx_mult=axx_mult, use_exact=USE_EXACT,sa_rows=r, sa_cols=c)\n",
    "    model.eval() \n",
    "\n",
    "    # Calibrate quickly (reuse same approach for fair comparison)\n",
    "    with torch.no_grad():\n",
    "        stats = collect_stats(model, data_t, num_batches=2)\n",
    "        amax = compute_amax(model, method=\"percentile\", percentile=99.99)    \n",
    "\n",
    "    # Warmup\n",
    "    xb, yb = next(iter(data))\n",
    "    _ = model(xb)\n",
    "\n",
    "    # Short run (10 batches)\n",
    "    iters = 10\n",
    "    start = timeit.default_timer()\n",
    "    with torch.no_grad():\n",
    "        for i, (x, _) in enumerate(data):\n",
    "            _ = model(x)\n",
    "            if i >= iters - 1:\n",
    "                break\n",
    "    t_small = timeit.default_timer() - start\n",
    "\n",
    "    # Full eval timing\n",
    "    start = timeit.default_timer()\n",
    "    acc = evaluate(model, data, desc=f\"Eval runtime SA {r}x{c}\")\n",
    "    t_full = timeit.default_timer() - start\n",
    "\n",
    "    results.append({\n",
    "        \"sa_rows\": r,\n",
    "        \"sa_cols\": c,\n",
    "        \"accuracy %\": acc,\n",
    "        \"time_10_batches_sec\": t_small,\n",
    "        \"time_full_eval_sec\": t_full,\n",
    "    })\n",
    "\n",
    "df_rt = pd.DataFrame(results).sort_values(by=[\"sa_rows\",\"sa_cols\"]).reset_index(drop=True)\n",
    "df_rt\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
