{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18ebda70",
   "metadata": {},
   "source": [
    "# # End-to-End: Build → Calibrate → (Re)Train → Compare Accuracy (16×16) → Compare Runtime (various SA)\n",
    "\n",
    "This notebook follows the same pattern as before, but bundles the whole workflow:\n",
    "\n",
    "1. **Build models** for two multipliers (`mul8s_acc` and `mul8s_1L2H`) and a fixed SA size (16×16).\n",
    "2. **Calibrate** both models (percentile histogram, short pass).\n",
    "3. **(Optional) Re-train** both models briefly (fine-tune) to reduce quantization/approx errors.\n",
    "4. **Compare accuracy** between the two 16×16 models (calibrated or fine-tuned).\n",
    "5. **Compare execution time** for a chosen multiplier across multiple SA sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8f70980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, timeit\n",
    "import torch\n",
    "import torchvision as tv\n",
    "from torchvision import transforms as T\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Safer dataloaders in constrained environments\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
    "\n",
    "# ---- Global config ----\n",
    "DEVICE = \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "NUM_CALIB_BATCHES = 2        # increase to 8–32 for better INT8 quality\n",
    "SA_ROWS, SA_COLS = 16, 16    # fixed for the accuracy comparison\n",
    "USE_EXACT = False            # True -> force exact mult; False -> use approx variants\n",
    "\n",
    "# Two variants to compare (16×16 accuracy)\n",
    "VARIANTS = [\"mul8s_acc\", \"mul8s_1L2H\"]\n",
    "\n",
    "# (Optional) training settings\n",
    "DO_FINETUNE = False          # set True to run a brief fine-tune \n",
    "FINETUNE_EPOCHS = 1\n",
    "LR = 1e-4\n",
    "WD = 0.0\n",
    "\n",
    "# SA configs for runtime comparison \n",
    "SA_CONFIGS = [(8,8), (16,16), (32,8), (8,32)]\n",
    "RUNTIME_MULT = \"mul8s_acc\"   # which multiplier to use for the runtime sweep\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 160)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fddaeb6",
   "metadata": {},
   "source": [
    "## Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aa60aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def val_dataloader(mean = (0.4914, 0.4822, 0.4465), std = (0.2471, 0.2435, 0.2616)):\n",
    "\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean, std),\n",
    "        ]\n",
    "    )\n",
    "    dataset = CIFAR10(root=\"datasets/cifar10_data\", train=False, download=True, transform=transform)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=True,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "transform = T.Compose(\n",
    "        [\n",
    "            T.RandomCrop(32, padding=4),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean = (0.4914, 0.4822, 0.4465), std = (0.2471, 0.2435, 0.2616)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "dataset = CIFAR10(root=\"datasets/cifar10_data\", train=True, download=True, transform=transform)\n",
    "\n",
    "evens = list(range(0, len(dataset), 10))\n",
    "trainset_1 = torch.utils.data.Subset(dataset, evens)\n",
    "\n",
    "data = val_dataloader()\n",
    "\n",
    "# data_t is used for calibration purposes and is a subset of train-set\n",
    "data_t = DataLoader(trainset_1, batch_size=128,\n",
    "                                            shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e637253",
   "metadata": {},
   "source": [
    "## Helpers (evaluate, calibration, amax, finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fc6531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_quantization.nn.modules.tensor_quantizer import TensorQuantizer\n",
    "from pytorch_quantization import calib\n",
    "from adapt.approx_layers.systolic_build import precompile_systolic_extensions\n",
    "from adapt.approx_layers.systolic_utils import swap_to_systolic\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import contextlib\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device=DEVICE, desc=\"Eval\"):\n",
    "    model.eval(); model.to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    model.eval()\n",
    "    start_time = timeit.default_timer()\n",
    "    with torch.no_grad():\n",
    "        for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "            images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(timeit.default_timer() - start_time)\n",
    "    print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))\n",
    "    return 100 * correct/total\n",
    "\n",
    "@torch.no_grad()\n",
    "def init_weight_amax_from_weights(model):\n",
    "    n_set, n_total = 0, 0\n",
    "    for _, m in model.named_modules():\n",
    "        q_w = getattr(m, \"quantizer_w\", None)\n",
    "        if isinstance(q_w, quant_nn.TensorQuantizer):\n",
    "            n_total += 1\n",
    "            if getattr(q_w, \"amax\", None) is None:\n",
    "                W = getattr(m, \"weight\", None)\n",
    "                if W is not None:\n",
    "                    q_w._amax = torch.as_tensor(W.detach().abs().max(), dtype=torch.float32)\n",
    "                    n_set += 1\n",
    "    print(f\"[init_weight_amax_from_weights] set {n_set}/{n_total} weight amax\")\n",
    "\n",
    "# 2) A small helper to attach pre-hooks that \"touch\" quantizers during calib\n",
    "def _make_calib_pre_hook(mod):\n",
    "    @torch.no_grad()\n",
    "    def _pre(mod_, inputs):\n",
    "        # inputs is a tuple; x is first positional input tensor\n",
    "        if not inputs:\n",
    "            return\n",
    "        x = inputs[0]\n",
    "        # Call activation quantizer to collect stats\n",
    "        q = getattr(mod_, \"quantizer\", None)\n",
    "        if isinstance(q, quant_nn.TensorQuantizer) and getattr(q, \"_calibrator\", None) is not None:\n",
    "            q(x)\n",
    "        # Touch weights as well so weight calibrator (if any) can record\n",
    "        q_w = getattr(mod_, \"quantizer_w\", None)\n",
    "        if isinstance(q_w, quant_nn.TensorQuantizer) and getattr(q_w, \"_calibrator\", None) is not None:\n",
    "            W = getattr(mod_, \"weight\", None)\n",
    "            if W is not None:\n",
    "                q_w(W)\n",
    "    return _pre\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def attach_calibration_hooks(model):\n",
    "    \"\"\"Attach forward-pre-hooks to every systolic layer so quantizers see tensors.\"\"\"\n",
    "    hooks = []\n",
    "    for _, m in model.named_modules():\n",
    "        # We only hook layers that have our quantizers AND a forward\n",
    "        has_any_q = isinstance(getattr(m, \"quantizer\", None), quant_nn.TensorQuantizer) or \\\n",
    "                    isinstance(getattr(m, \"quantizer_w\", None), quant_nn.TensorQuantizer)\n",
    "        if has_any_q:\n",
    "            try:\n",
    "                h = m.register_forward_pre_hook(_make_calib_pre_hook(m))\n",
    "                hooks.append(h)\n",
    "            except Exception:\n",
    "                pass\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        for h in hooks:\n",
    "            try:\n",
    "                h.remove()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "# 3) Fixed collect_stats using the hooks\n",
    "def collect_stats(model, data_loader, num_batches=10, device=\"cpu\"):\n",
    "    \"\"\"Collect activation histograms. Uses pre-hooks to ensure quantizers see tensors.\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Enable calibration (disable quantization) so quantizers record histograms\n",
    "    for _, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                module.disable_quant()\n",
    "                module.enable_calib()\n",
    "            else:\n",
    "                module.disable()\n",
    "\n",
    "    with torch.no_grad(), attach_calibration_hooks(model):\n",
    "        for i, (image, _) in enumerate(data_loader):\n",
    "            image = image.to(device, non_blocking=True)\n",
    "            _ = model(image)  # pre-hooks will call quantizers here\n",
    "            if i >= num_batches - 1:\n",
    "                break\n",
    "\n",
    "    # Disable calibration (enable quantization for inference)\n",
    "    for _, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                module.enable_quant()\n",
    "                module.disable_calib()\n",
    "            else:\n",
    "                module.enable()\n",
    "\n",
    "    print(\"Calibration data collection complete.\")\n",
    "\n",
    "# 4) Compute and sanitize amax \n",
    "def compute_amax(model, method=\"percentile\", percentile=99.99, strict=False, fallback=1.0):\n",
    "    n_loaded, n_fixed = 0, 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                try:\n",
    "                    if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "                        module.load_calib_amax(strict=strict)\n",
    "                    else:\n",
    "                        module.load_calib_amax(method=method, percentile=percentile, strict=strict)\n",
    "                    n_loaded += 1\n",
    "                except RuntimeError:\n",
    "                    pass\n",
    "            # sanitize\n",
    "            amax = getattr(module, \"amax\", None)\n",
    "            if (amax is None) or (torch.isnan(amax)) or (float(amax) == 0.0):\n",
    "                module._amax = torch.tensor(float(fallback), dtype=torch.float32)\n",
    "                n_fixed += 1\n",
    "            print(F\"{name:40}: {module}\")\n",
    "    print(f\"Loaded calibrated amax values. loaded={n_loaded}, sanitized={n_fixed}\")\n",
    "    model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d74a7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import calib\n",
    "from adapt.references.classification.train import train_one_epoch, load_data\n",
    "def finetune_one_epoch(model, loader, device=DEVICE, lr=1e-4, wd=0.0):\n",
    "    from pytorch_quantization import nn as quant_nn\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "    \n",
    "    # finetune the model for one epoch based on data_t subset \n",
    "    train_one_epoch(model, criterion, optimizer, loader, device, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5956b04c",
   "metadata": {},
   "source": [
    "## Accuracy comparison ( systolic array size 16×16) for two multipliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae3eca79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== resnet50_systolic @16x16: mul8s_acc ===\n",
      "Pre-compiling systolic extensions for mul8s_acc...\n",
      "  Mode: approx\n",
      "    • linear (r16×c16)\n",
      "    • conv2d (r16×c16)\n",
      "Pre-compilation complete! Models will now load instantly.\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_acc, exact=False, SA=16x16\n",
      "✓ Loaded systolic linear kernel: mul8s_acc, exact=False, SA=16x16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1109 22:56:11.721399 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.722329 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.722698 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.723043 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.723437 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.723791 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.724147 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.724495 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.724864 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.725259 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.725633 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.726020 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.726369 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.726762 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.727120 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.727443 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.727783 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.728099 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.728491 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.728806 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.729119 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.729424 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.729743 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.730072 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.730402 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.730707 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.731013 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.731312 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.731630 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.731934 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.732248 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.732555 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.732868 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.733180 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.733486 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.733775 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.734105 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.734468 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.734820 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.735166 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.735482 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.735785 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.736096 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.736398 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.736704 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.736994 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.737357 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.737686 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.738013 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.738328 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.738692 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.739057 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.739452 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.743118 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.743394 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.743692 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.744017 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.744429 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.744760 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.745052 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.745330 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.745603 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.745887 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.746150 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.746515 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.746856 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.747223 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.747559 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.747886 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.748165 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.748447 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.748735 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.749016 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.749291 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.749572 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.749848 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.750125 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.750397 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.750671 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.750946 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.751227 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.751500 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.751860 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.752146 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.752435 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.754859 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.755234 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.755545 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.755846 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.756134 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1109 22:56:11.756426 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.756732 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.757040 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.757330 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.757620 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.757904 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.758187 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.758488 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.758776 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.759063 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.759372 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.759657 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.760025 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.760329 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.760620 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.760948 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.761221 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.761485 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 22:56:11.763593 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.763936 140511505602368 tensor_quantizer.py:238] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "W1109 22:56:11.764666 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.765479 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.766163 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.766849 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.767424 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.767976 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.768518 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.769179 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.769750 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.770246 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.770990 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.771600 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.772154 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.772730 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.773284 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.773847 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.774404 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.774962 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.775500 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.776025 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.776541 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.776957 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.777703 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.778279 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.778804 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.779194 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.779877 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.780277 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.781058 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.781647 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.782342 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.782968 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.783603 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.784218 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.784838 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.785440 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.786046 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.786746 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.787354 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.787938 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.788500 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.789086 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.789637 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.790217 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.790782 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.791358 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.791885 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.792411 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.792973 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.793499 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.794092 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.794686 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.795342 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.795996 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.796570 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.797183 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.797736 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.798297 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.798849 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.799407 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1109 22:56:11.799958 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.800517 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.801116 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.801682 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.802239 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.802794 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.803343 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.803899 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.804484 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.805102 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.805611 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.806182 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.806755 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.807314 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.807830 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.808391 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.809076 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.809700 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.810292 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.810888 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.811472 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.812086 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.812721 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.813297 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.813844 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.814446 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.815011 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.815563 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.816088 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.816451 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.817146 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.817715 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.818269 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.818851 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.819390 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.819973 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.820608 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.821242 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.821810 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.822363 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.822902 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.823499 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.824023 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.824574 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.825126 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.825671 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 22:56:11.826197 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration data collection complete.\n",
      "conv1.quantizer                         : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator quant)\n",
      "conv1.quantizer_w                       : TensorQuantizer(8bit per-tensor amax=0.1625 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.6236 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0804 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2153 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0355 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2416 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0544 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.6236 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0867 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2282 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0270 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1354 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0287 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1639 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0338 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2301 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0412 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1685 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0334 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3141 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0479 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2357 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0423 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1705 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0212 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2025 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0298 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.2357 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0273 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1462 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0200 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1400 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0167 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1553 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0267 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1449 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0243 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1431 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0189 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1592 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0303 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1464 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0273 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1472 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0215 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1693 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0277 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1593 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0266 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1388 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0128 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1855 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0160 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.1593 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0104 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0989 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0125 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0955 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0107 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0932 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0177 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1087 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0122 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0835 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0861 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0172 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1217 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0111 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1018 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0081 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1294 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0151 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1393 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0125 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1092 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0084 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1383 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0161 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1930 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0124 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1136 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0068 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1451 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0185 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4988 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0093 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0830 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0029 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2676 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0087 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4988 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0131 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5305 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0029 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1474 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0040 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3064 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0107 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.8090 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0024 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1316 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0042 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3075 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0111 calibrator=HistogramCalibrator quant)\n",
      "fc.quantizer                            : TensorQuantizer(8bit per-tensor amax=1.0809 calibrator=HistogramCalibrator quant)\n",
      "fc.quantizer_w                          : TensorQuantizer(8bit per-tensor amax=0.1751 calibrator=HistogramCalibrator quant)\n",
      "Loaded calibrated amax values. loaded=108, sanitized=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 78/78 [48:33<00:00, 37.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2913.214075219992\n",
      "Accuracy of the network on the 10000 test images: 93.6498 %\n",
      "=== resnet50_systolic @16x16: mul8s_1L2H ===\n",
      "Pre-compiling systolic extensions for mul8s_1L2H...\n",
      "  Mode: approx\n",
      "    • linear (r16×c16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    • conv2d (r16×c16)\n",
      "Pre-compilation complete! Models will now load instantly.\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic linear kernel: mul8s_1L2H, exact=False, SA=16x16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1109 23:45:31.566247 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.566863 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.567243 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.567556 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.567908 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.568291 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.568665 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.569074 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.569480 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.569818 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.570209 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.570535 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.570852 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.571166 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.571482 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.571823 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.572146 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.572455 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.572777 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.573120 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.573447 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.573825 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.576759 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.577383 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.577831 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.578291 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.578767 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.579290 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.579791 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.580321 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.580802 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.581224 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.581635 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.582211 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.582622 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.583029 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.583370 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.583676 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.583996 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.584280 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.584562 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.584883 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.585328 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.585855 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.586243 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.586607 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.586984 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.587319 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.587671 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.588063 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.588502 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.588973 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.589453 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.589885 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.590776 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.593468 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.593818 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.594235 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.594565 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.594842 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.595111 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.595389 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.595670 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.595952 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.596236 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.596512 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.596803 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.597097 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.597395 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.597684 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.598112 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.598501 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.598896 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.599276 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.599644 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.600004 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.602353 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.603049 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.603422 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.603849 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.604262 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.604731 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.605188 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.605681 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.606244 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.606680 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.607084 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.607462 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.607839 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.608210 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.608582 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1109 23:45:31.608962 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.609355 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.609710 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.610060 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.610431 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.610807 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.611179 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.611558 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.611917 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.612283 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.612644 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.613016 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.613377 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.613741 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.614106 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.614473 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.614842 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1109 23:45:31.619182 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.619967 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.620478 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.621469 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.622447 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.623231 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.624055 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.624965 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.625841 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.626640 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.627507 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.628960 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.630074 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.631366 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.632411 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.633274 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.634104 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.635023 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.635916 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.636626 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.637408 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.638166 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.639025 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.639845 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.640644 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.641452 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.642214 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.642967 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.643725 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.644462 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.645215 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.645928 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.646661 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.647423 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.648261 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.649088 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.649881 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.650620 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.651363 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.652108 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.652959 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.653800 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.654594 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.655367 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.656150 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.656986 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.657855 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.658711 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.659556 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.660473 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.661952 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.663253 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.664507 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.665533 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.666458 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.667334 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.668190 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.669045 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.669946 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.670826 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.671847 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.672835 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.673700 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1109 23:45:31.674472 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.675251 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.676008 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.676821 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.677601 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.678378 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.679055 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.679596 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.680127 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.680663 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.681499 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.682234 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.682690 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.683441 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.683960 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.684592 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.685319 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.686132 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.687009 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.687933 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.688799 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.689612 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.690416 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.691305 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.692152 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.693055 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.693880 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.694567 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.695290 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.696252 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.697369 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.698760 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.699937 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.700857 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.701725 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.702518 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.703313 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.704278 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.705218 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.706259 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.707381 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.708270 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.709121 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.709931 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1109 23:45:31.710550 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration data collection complete.\n",
      "conv1.quantizer                         : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator quant)\n",
      "conv1.quantizer_w                       : TensorQuantizer(8bit per-tensor amax=0.1625 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.6256 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0804 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2144 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0355 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2372 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0544 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.6256 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0867 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2263 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0270 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1326 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0287 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1643 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0338 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2278 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0412 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1761 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0334 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3286 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0479 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2315 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0423 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1709 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0212 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2027 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0298 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.2315 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0273 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1460 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0200 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1395 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0167 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1565 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0267 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1448 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0243 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1431 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0189 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1584 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0303 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1464 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0273 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1453 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0215 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1705 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0277 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1587 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0266 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1390 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0128 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1849 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0160 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.1587 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0104 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0962 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0125 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0969 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0107 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0916 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0177 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1073 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0122 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0826 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0861 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0172 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1191 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0111 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0974 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0081 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1280 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0151 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1366 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0125 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1064 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0084 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1336 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0161 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1908 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0124 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1150 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0068 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1450 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0185 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4965 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0093 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0833 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0029 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2677 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0087 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4965 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0131 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5313 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0029 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1443 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0040 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3058 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0107 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.8030 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0024 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1300 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0042 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3066 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0111 calibrator=HistogramCalibrator quant)\n",
      "fc.quantizer                            : TensorQuantizer(8bit per-tensor amax=1.0552 calibrator=HistogramCalibrator quant)\n",
      "fc.quantizer_w                          : TensorQuantizer(8bit per-tensor amax=0.1751 calibrator=HistogramCalibrator quant)\n",
      "Loaded calibrated amax values. loaded=108, sanitized=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 78/78 [48:07<00:00, 37.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2887.380255897995\n",
      "Accuracy of the network on the 10000 test images: 82.6623 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variant</th>\n",
       "      <th>Type</th>\n",
       "      <th>Accuracy %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mul8s_1L2H</td>\n",
       "      <td>calibrated</td>\n",
       "      <td>82.66226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mul8s_acc</td>\n",
       "      <td>calibrated</td>\n",
       "      <td>93.64984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Variant        Type  Accuracy %\n",
       "0  mul8s_1L2H  calibrated    82.66226\n",
       "1   mul8s_acc  calibrated    93.64984"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Accuracy comparison @16x16 using resnet50_systolic ===\n",
    "rows = []\n",
    "\n",
    "try:\n",
    "    from models.resnet_systolic import resnet50_systolic\n",
    "except Exception:\n",
    "    from examples.models.resnet_systolic import resnet50_systolic\n",
    "\n",
    "for axx_mult in VARIANTS:  # z.B. [\"mul8s_acc\", \"mul8s_1L2H\"]\n",
    "    print(f\"=== resnet50_systolic @16x16: {axx_mult} ===\")\n",
    "\n",
    "    # 1) Precompile \n",
    "    precompile_systolic_extensions(\n",
    "        axx_mult=axx_mult,\n",
    "        use_exact_variants=(USE_EXACT,),\n",
    "        sa_rows=16, sa_cols=16,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    model = resnet50_systolic(pretrained=True, axx_mult=axx_mult, use_exact=USE_EXACT)\n",
    "    model.eval()  \n",
    "\n",
    "    # 3) calibration\n",
    "    with torch.no_grad():\n",
    "        #init_weight_amax_from_weights(model)  \n",
    "        stats = collect_stats(model, data_t, num_batches=2)\n",
    "        amax = compute_amax(model, method=\"percentile\", percentile=99.99)\n",
    "    \n",
    "    # optional - test different calibration methods\n",
    "    #amax = compute_amax(model, method=\"mse\")\n",
    "    #amax = compute_amax(model, method=\"entropy\")\n",
    "\n",
    "\n",
    "    acc_cal = evaluate(model, data, DEVICE, desc=f\"Eval resnet50_systolic ({axx_mult}) calibrated\")\n",
    "    rows.append({\"Variant\": axx_mult, \"Type\": \"calibrated\", \"Accuracy %\": acc_cal})\n",
    "\n",
    "    import torch.nn as nn\n",
    "\n",
    "    if DO_FINETUNE:\n",
    "        \n",
    "        for epoch in range(FINETUNE_EPOCHS):\n",
    "            finetune_one_epoch(model, data_t, lr=LR, wd=WD)\n",
    "        model.eval()\n",
    "        acc_ft = evaluate(model, data, 'cpu',desc=f\"Eval resnet50_systolic ({axx_mult}) finetuned\")\n",
    "        rows.append({\"Variant\": axx_mult, \"Type\": \"finetuned\", \"Accuracy %\": acc_ft})\n",
    "\n",
    "df_acc = pd.DataFrame(rows).sort_values(by=[\"Variant\", \"Type\"]).reset_index(drop=True)\n",
    "df_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f9d5ed",
   "metadata": {},
   "source": [
    "## Runtime comparison across SA sizes (same multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a30d2e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Runtime: mul8s_acc @ SA 8x8 ===\n",
      "Pre-compiling systolic extensions for mul8s_acc...\n",
      "  Mode: approx\n",
      "    • linear (r8×c8)\n",
      "    • conv2d (r8×c8)\n",
      "Pre-compilation complete! Models will now load instantly.\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x8\n",
      "✓ Loaded systolic linear kernel: mul8s_1L2H, exact=False, SA=8x8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 00:34:38.784360 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.785006 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.785623 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.785949 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.786425 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.786763 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.787105 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.787488 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.787899 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.788851 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.789342 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.789676 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.790037 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.790630 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.790968 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.793724 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.796690 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.797210 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.797684 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.798194 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.798640 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.799075 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.799512 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.799968 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.807285 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.808062 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.809138 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.809950 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.810622 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.811188 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.811686 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.812114 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.812676 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.813185 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.813743 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.814288 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.815897 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.816444 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.816941 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.817435 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.819720 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.820206 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.820674 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.821121 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.821581 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.822024 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.822463 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.822899 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.823348 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.823797 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.824243 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.824674 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.825164 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.825629 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.826048 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.826585 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.827067 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.827453 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.827859 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.828369 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.828850 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.829298 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.829758 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.830192 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.830627 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.831047 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.831471 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.831886 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.832336 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.832824 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.833311 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.833831 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.834340 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.834766 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.835184 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.835597 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.836024 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.836419 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.836866 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.841981 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.842414 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.842839 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.843314 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.843724 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.844180 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.844633 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.845101 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.845549 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.845998 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.846449 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.846918 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 00:34:38.847416 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.849559 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.850072 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.850532 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.850883 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.851232 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.851575 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.851920 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.852258 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.852573 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.852882 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.853188 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.853464 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.853742 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.854030 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.854358 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.854762 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 00:34:38.857251 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.858201 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.859353 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.860072 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.860718 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.861344 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.861970 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.862566 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.863158 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.863733 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.864317 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.864906 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.865499 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.866091 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.866813 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.867604 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.868357 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.868979 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.869571 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.870151 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.870736 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.871624 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.872245 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.872840 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.873479 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.874107 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.874645 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.875254 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.875900 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.876514 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.877113 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.877683 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.878327 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.879081 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.879940 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.880994 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.882005 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.883060 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.883792 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.884378 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.884953 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.885501 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.886060 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.886614 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.887261 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.888149 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.889109 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.889886 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.890596 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.891256 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.891908 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.892538 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.893340 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.894539 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.895443 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.896636 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.897597 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.898501 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.899160 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.899765 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.900379 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.901150 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.901918 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 00:34:38.902639 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.903575 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.904795 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.905853 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.906650 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.907408 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.908141 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.909008 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.909872 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.910739 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.911861 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.912893 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.913650 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.914355 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.915065 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.915772 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.916454 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.917169 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.917868 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.918450 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.919023 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.919584 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.920232 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.920928 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.921532 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.922407 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.923094 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.923729 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.924350 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.925013 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.925710 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.926549 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.927154 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.927734 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.928305 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.928884 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.929443 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.930019 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.930568 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.931227 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.931940 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.932664 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.933413 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.934150 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 00:34:38.934863 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration data collection complete.\n",
      "conv1.quantizer                         : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator quant)\n",
      "conv1.quantizer_w                       : TensorQuantizer(8bit per-tensor amax=0.1625 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.6284 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0804 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2148 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0355 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2408 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0544 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.6284 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0867 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2253 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0270 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1362 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0287 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1652 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0338 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2269 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0412 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1764 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0334 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3251 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0479 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2316 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0423 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1704 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0212 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2018 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0298 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.2316 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0273 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1460 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0200 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1448 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0167 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1522 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0267 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1452 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0243 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1447 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0189 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1596 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0303 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1464 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0273 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1460 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0215 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1756 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0277 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1588 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0266 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1398 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0128 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1923 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0160 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.1588 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0104 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0977 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0125 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0922 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0107 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0925 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0177 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1084 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0122 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0841 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0878 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0172 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1208 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0111 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1008 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0081 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1309 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0151 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1417 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0125 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1074 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0084 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1346 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0161 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1910 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0124 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1140 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0068 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1456 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0185 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4941 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0093 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0826 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0029 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2658 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0087 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4941 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0131 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5360 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0029 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1462 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0040 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3105 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0107 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.8121 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0024 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1325 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0042 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3084 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0111 calibrator=HistogramCalibrator quant)\n",
      "fc.quantizer                            : TensorQuantizer(8bit per-tensor amax=1.0733 calibrator=HistogramCalibrator quant)\n",
      "fc.quantizer_w                          : TensorQuantizer(8bit per-tensor amax=0.1751 calibrator=HistogramCalibrator quant)\n",
      "Loaded calibrated amax values. loaded=108, sanitized=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 78/78 [48:33<00:00, 37.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2913.2589172069565\n",
      "Accuracy of the network on the 10000 test images: 82.6322 %\n",
      "=== Runtime: mul8s_acc @ SA 16x16 ===\n",
      "Pre-compiling systolic extensions for mul8s_acc...\n",
      "  Mode: approx\n",
      "    • linear (r16×c16)\n",
      "    • conv2d (r16×c16)\n",
      "Pre-compilation complete! Models will now load instantly.\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=16x16\n",
      "✓ Loaded systolic linear kernel: mul8s_1L2H, exact=False, SA=16x16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 01:30:09.889962 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.890531 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.890904 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.891244 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.891607 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.891974 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.892364 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.893326 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.893754 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.894114 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.894486 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.894827 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.895193 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.895537 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.895897 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.896682 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.897184 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.897834 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.898699 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.899092 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.899824 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.900201 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.900628 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.901124 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.901571 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.902004 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.902529 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.903055 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.904731 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.905931 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.906387 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.906812 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.907274 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.907690 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.907999 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.908311 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.908623 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.908935 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.909245 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.909584 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.909914 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.910233 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.917218 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.917699 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.918084 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.918434 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.918767 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.919155 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.919580 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.921501 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.921932 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.922294 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.922642 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.922993 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.923380 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.923701 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.924003 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.924307 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.924624 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.925008 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.925358 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.925712 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.926097 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.926431 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.926741 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.927040 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.927332 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.927622 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.927922 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.928222 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.928506 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.928810 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.929096 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.929378 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.929664 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.929945 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.930291 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.930627 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.930997 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.931284 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.931569 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.931846 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.932137 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.932405 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.932680 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.932951 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.933257 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.933548 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.933839 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.934113 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.934449 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 01:30:09.934780 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.935157 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.935513 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.935805 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.936089 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.936408 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.936837 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.937220 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.937512 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.937847 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.938169 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.938508 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.938862 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.939154 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.939472 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.939767 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.940049 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 01:30:09.944839 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.945728 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.946382 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.946981 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.947829 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.948422 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.949015 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.949572 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.950270 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.950970 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.951677 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.952370 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.953137 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.954198 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.954936 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.955696 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.956488 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.957262 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.957861 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.958431 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.959107 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.959637 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.960249 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.960936 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.961622 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.962289 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.962993 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.963736 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.964505 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.965373 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.966180 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.966948 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.967702 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.968388 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.969043 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.969627 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.970229 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.970941 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.971602 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.972277 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.973021 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.973810 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.974637 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.975278 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.975848 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.976396 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.976988 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.977552 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.978120 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.978672 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.979230 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.979774 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.980320 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.980880 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.981434 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.981983 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.982549 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.983232 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.984062 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.984677 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.985306 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.985861 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.986449 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 01:30:09.987018 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.987565 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.988165 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.988955 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.989707 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.990428 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.991122 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.991805 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.992480 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.993412 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.994185 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.995038 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.995948 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.996982 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.997933 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.998522 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.999055 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:09.999659 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:10.000219 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:10.000976 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:10.001552 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:10.002147 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:10.002725 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:10.003294 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:10.003822 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:10.004537 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:10.005266 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:10.006211 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:10.007223 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:10.007894 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:10.008477 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:10.009075 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:10.009766 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:10.010344 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:10.010885 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:10.011442 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:10.012162 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:10.012723 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:10.013254 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:10.013783 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:10.014484 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:10.015247 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:10.016283 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:10.017107 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 01:30:10.018003 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration data collection complete.\n",
      "conv1.quantizer                         : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator quant)\n",
      "conv1.quantizer_w                       : TensorQuantizer(8bit per-tensor amax=0.1625 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.6102 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0804 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2118 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0355 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2406 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0544 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.6102 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0867 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2282 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0270 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1349 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0287 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1653 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0338 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2283 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0412 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1739 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0334 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3266 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0479 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2309 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0423 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1707 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0212 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2074 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0298 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.2309 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0273 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1466 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0200 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1421 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0167 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1558 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0267 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1452 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0243 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1426 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0189 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1568 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0303 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1451 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0273 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1502 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0215 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1735 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0277 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1584 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0266 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1385 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0128 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1858 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0160 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.1584 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0104 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0976 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0125 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0944 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0107 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0909 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0177 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1083 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0122 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0839 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0865 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0172 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1210 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0111 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0995 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0081 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1285 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0151 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1359 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0125 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1081 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0084 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1375 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0161 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1902 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0124 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1152 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0068 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1465 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0185 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4988 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0093 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0828 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0029 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2732 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0087 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4988 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0131 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5374 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0029 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1445 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0040 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3075 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0107 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.8075 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0024 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1313 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0042 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3057 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0111 calibrator=HistogramCalibrator quant)\n",
      "fc.quantizer                            : TensorQuantizer(8bit per-tensor amax=1.0637 calibrator=HistogramCalibrator quant)\n",
      "fc.quantizer_w                          : TensorQuantizer(8bit per-tensor amax=0.1751 calibrator=HistogramCalibrator quant)\n",
      "Loaded calibrated amax values. loaded=108, sanitized=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 78/78 [48:11<00:00, 37.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2891.5949548639474\n",
      "Accuracy of the network on the 10000 test images: 82.5321 %\n",
      "=== Runtime: mul8s_acc @ SA 32x8 ===\n",
      "Pre-compiling systolic extensions for mul8s_acc...\n",
      "  Mode: approx\n",
      "    • linear (r32×c8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    • conv2d (r32×c8)\n",
      "Pre-compilation complete! Models will now load instantly.\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=32x8\n",
      "✓ Loaded systolic linear kernel: mul8s_1L2H, exact=False, SA=32x8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 02:26:07.914930 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.915636 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.916093 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.916422 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.916798 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.917155 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.917540 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.917905 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.918276 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.918670 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.919056 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.919396 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.919741 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.920076 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.920423 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.920769 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.921750 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.922323 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.922661 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.922964 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.923263 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.923552 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.923861 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.924160 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.924468 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.924837 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.925230 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.925649 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.926040 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.927002 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.927544 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.927900 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.928250 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.928589 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.929284 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.929662 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.930128 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.933377 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.933772 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.934122 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.934566 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.934940 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.935355 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.935679 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.936044 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.936362 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.936745 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.937109 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.937463 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.937779 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.938099 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.938708 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.940246 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.940801 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.941306 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.942140 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.942597 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.943055 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.943383 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.943675 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.943968 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.944272 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.944578 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.944907 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.945240 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.945530 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.945819 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.946114 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.946469 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.946810 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.947207 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.947510 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.947804 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.948087 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.948385 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.948696 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.949018 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.949309 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.949600 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.949885 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.950187 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.950474 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.950784 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.951115 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.951390 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.951658 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.951932 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.952197 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.952478 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.955463 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.955901 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 02:26:07.956231 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.956597 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.957030 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.957439 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.957890 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.958314 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.958611 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.958884 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.959146 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.959413 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.959674 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.959939 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.960198 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.960808 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.961070 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.961339 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.961598 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 02:26:07.963334 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.963910 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.964445 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.965026 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.965551 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.966085 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.966609 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.967132 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.967497 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.968314 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.969068 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.969784 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.970523 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.971349 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.972094 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.972907 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.973614 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.974373 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.975521 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.976663 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.977877 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.978796 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.979553 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.980203 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.980860 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.981506 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.982135 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.982758 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.983392 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.984014 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.984729 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.985376 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.986026 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.986705 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.987617 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.988905 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.989763 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.990726 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.991658 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.992355 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.993012 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.993655 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.994283 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.994892 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.995510 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.996117 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.997118 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.997826 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.998506 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.999155 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:07.999761 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.000706 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.001667 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.002416 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.003371 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.004076 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.004765 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.005422 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.006368 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.007118 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.007841 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.008570 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.009362 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 02:26:08.010156 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.010806 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.011392 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.011968 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.012531 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.013099 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.013670 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.014227 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.014784 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.015330 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.015880 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.016938 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.017516 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.018202 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.018814 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.019439 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.020097 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.020798 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.021530 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.022414 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.023234 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.024224 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.025095 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.025749 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.026304 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.026841 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.027388 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.027941 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.028481 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.029038 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.029581 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.030098 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.030606 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.031146 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.031664 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.033056 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.033730 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.034475 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.035098 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.035759 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.036375 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.037032 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.037640 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.038230 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 02:26:08.038811 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration data collection complete.\n",
      "conv1.quantizer                         : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator quant)\n",
      "conv1.quantizer_w                       : TensorQuantizer(8bit per-tensor amax=0.1625 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.6144 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0804 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2157 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0355 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2420 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0544 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.6144 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0867 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2264 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0270 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1360 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0287 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1648 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0338 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2272 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0412 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1757 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0334 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3362 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0479 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2310 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0423 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1728 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0212 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2005 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0298 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.2310 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0273 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1468 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0200 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1404 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0167 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1549 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0267 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1453 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0243 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1430 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0189 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1565 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0303 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1467 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0273 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1464 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0215 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1746 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0277 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1596 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0266 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1402 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0128 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1914 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0160 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.1596 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0104 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0997 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0125 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0952 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0107 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0918 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0177 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1082 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0122 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0843 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0852 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0172 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1225 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0111 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1022 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0081 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1345 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0151 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1395 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0125 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1071 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0084 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1360 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0161 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1935 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0124 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1172 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0068 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1482 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0185 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4999 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0093 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0833 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0029 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2661 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0087 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4999 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0131 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5276 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0029 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1453 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0040 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3070 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0107 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.8044 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0024 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1315 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0042 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3055 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0111 calibrator=HistogramCalibrator quant)\n",
      "fc.quantizer                            : TensorQuantizer(8bit per-tensor amax=1.0744 calibrator=HistogramCalibrator quant)\n",
      "fc.quantizer_w                          : TensorQuantizer(8bit per-tensor amax=0.1751 calibrator=HistogramCalibrator quant)\n",
      "Loaded calibrated amax values. loaded=108, sanitized=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 78/78 [48:20<00:00, 37.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2900.727436061017\n",
      "Accuracy of the network on the 10000 test images: 82.5321 %\n",
      "=== Runtime: mul8s_acc @ SA 8x32 ===\n",
      "Pre-compiling systolic extensions for mul8s_acc...\n",
      "  Mode: approx\n",
      "    • linear (r8×c32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    • conv2d (r8×c32)\n",
      "Pre-compilation complete! Models will now load instantly.\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic conv2d kernel: mul8s_1L2H, exact=False, SA=8x32\n",
      "✓ Loaded systolic linear kernel: mul8s_1L2H, exact=False, SA=8x32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 03:22:22.894865 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.895585 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.895973 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.896524 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.896927 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.897255 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.897606 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.898334 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.898712 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.899238 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.899705 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.900141 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.900606 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.901054 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.901514 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.901989 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.902421 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.902836 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.903625 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.904037 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.904407 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.905123 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.905586 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.906586 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.907158 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.907860 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.908306 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.908672 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.909056 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.909440 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.909791 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.910370 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.910722 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.911095 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.911416 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.911865 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.912268 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.912643 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.913032 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.913391 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.913733 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.914073 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.914396 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.914772 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.915097 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.915481 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.915869 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.916285 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.916716 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.917089 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.917422 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.917724 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.918070 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.918399 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.918762 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.919268 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.919680 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.920095 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.920547 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.921021 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.921446 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.921862 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.922318 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.922729 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.923096 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.923442 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.923800 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.924143 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.924494 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.925100 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.925454 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.925797 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.926145 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.926493 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.926847 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.927197 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.927544 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.927884 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.928236 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.928579 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.928986 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.929332 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.929685 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.930029 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.930387 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.930734 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.931090 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.931432 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.931776 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.932152 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.932519 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 03:22:22.932885 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.933247 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.933597 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.933949 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.934287 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.934620 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.934948 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.935307 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.935695 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.936330 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.936858 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.937240 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.937605 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.937963 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.943697 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.944166 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.944517 140511505602368 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1110 03:22:22.945528 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.946402 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.947245 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.948119 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.949134 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.950167 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.950985 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.951745 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.952445 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.953124 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.953819 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.954558 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.955341 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.956101 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.956996 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.957936 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.958686 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.959372 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.960357 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.961061 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.961745 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.962379 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.963068 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.963695 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.964340 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.964977 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.965878 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.966592 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.967447 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.968027 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.968755 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.969454 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.970172 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.970846 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.971447 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.972014 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.972582 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.973035 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.974036 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.975382 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.976222 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.977138 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.978008 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.978638 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.979201 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.979744 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.980281 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.980816 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.981418 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.981966 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.982610 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.983189 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.983753 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.984309 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.984888 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.985439 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.985999 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.986548 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.987002 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.987890 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.988577 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.989384 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.990277 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 03:22:22.990981 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.991612 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.992199 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.992830 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.993439 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.994024 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.994629 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.995213 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.995835 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.996454 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.997089 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.997848 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.998476 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.999073 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:22.999728 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.000395 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.001049 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.001724 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.002397 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.003060 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.003706 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.004348 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.004985 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.005644 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.006291 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.006935 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.007575 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.008237 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.009195 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.010161 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.011337 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.012317 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.013030 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.013623 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.014187 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.014755 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.015418 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.016020 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.016575 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.017180 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.017737 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.018295 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.018872 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.019454 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1110 03:22:23.020035 140511505602368 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration data collection complete.\n",
      "conv1.quantizer                         : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator quant)\n",
      "conv1.quantizer_w                       : TensorQuantizer(8bit per-tensor amax=0.1625 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.6045 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0804 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2141 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0355 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2437 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0544 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.6045 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0867 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2270 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0270 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1362 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0287 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1633 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0338 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2276 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0412 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1713 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0334 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3146 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0479 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2326 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0423 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1720 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0212 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2006 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0298 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.2326 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0273 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1471 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0200 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1437 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0167 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1550 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0267 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1453 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0243 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1426 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0189 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1601 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0303 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1474 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0273 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1496 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0215 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1715 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0277 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1595 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0266 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1389 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0128 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1909 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0160 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.1595 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0104 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0978 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0125 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0952 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0107 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0921 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0177 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1075 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0122 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0833 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0873 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0172 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1202 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0111 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0988 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0081 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1239 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0151 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1374 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0125 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1068 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0084 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1368 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0161 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1904 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0124 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1152 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0068 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1475 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0185 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4958 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0093 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0830 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0029 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2640 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0087 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4958 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0131 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5321 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0029 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1430 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0040 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3062 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0107 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.8056 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0024 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1310 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0042 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3069 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0111 calibrator=HistogramCalibrator quant)\n",
      "fc.quantizer                            : TensorQuantizer(8bit per-tensor amax=1.0621 calibrator=HistogramCalibrator quant)\n",
      "fc.quantizer_w                          : TensorQuantizer(8bit per-tensor amax=0.1751 calibrator=HistogramCalibrator quant)\n",
      "Loaded calibrated amax values. loaded=108, sanitized=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 78/78 [48:20<00:00, 37.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2900.0329998249654\n",
      "Accuracy of the network on the 10000 test images: 82.5220 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sa_rows</th>\n",
       "      <th>sa_cols</th>\n",
       "      <th>accuracy %</th>\n",
       "      <th>time_10_batches_sec</th>\n",
       "      <th>time_full_eval_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>82.632212</td>\n",
       "      <td>372.673052</td>\n",
       "      <td>2913.261563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>82.522035</td>\n",
       "      <td>366.461684</td>\n",
       "      <td>2900.035655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>82.532051</td>\n",
       "      <td>369.976510</td>\n",
       "      <td>2891.597569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>82.532051</td>\n",
       "      <td>374.428506</td>\n",
       "      <td>2900.730295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sa_rows  sa_cols  accuracy %  time_10_batches_sec  time_full_eval_sec\n",
       "0        8        8   82.632212           372.673052         2913.261563\n",
       "1        8       32   82.522035           366.461684         2900.035655\n",
       "2       16       16   82.532051           369.976510         2891.597569\n",
       "3       32        8   82.532051           374.428506         2900.730295"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We compare elapsed time for a short pass (10 batches) and full evaluation.\n",
    "results = []\n",
    "\n",
    "for (r, c) in SA_CONFIGS:\n",
    "    print(f\"=== Runtime: {RUNTIME_MULT} @ SA {r}x{c} ===\")\n",
    "    precompile_systolic_extensions(axx_mult=RUNTIME_MULT, use_exact_variants=(USE_EXACT,),\n",
    "                                   sa_rows=r, sa_cols=c, verbose=False)\n",
    "    \n",
    "    model = resnet50_systolic(pretrained=True, axx_mult=axx_mult, use_exact=USE_EXACT,sa_rows=r, sa_cols=c)\n",
    "    model.eval() \n",
    "\n",
    "    # Calibrate quickly (reuse same approach for fair comparison)\n",
    "    with torch.no_grad():\n",
    "        stats = collect_stats(model, data_t, num_batches=2)\n",
    "        amax = compute_amax(model, method=\"percentile\", percentile=99.99)    \n",
    "\n",
    "    # Warmup\n",
    "    xb, yb = next(iter(data))\n",
    "    _ = model(xb)\n",
    "\n",
    "    # Short run (10 batches)\n",
    "    iters = 10\n",
    "    start = timeit.default_timer()\n",
    "    with torch.no_grad():\n",
    "        for i, (x, _) in enumerate(data):\n",
    "            _ = model(x)\n",
    "            if i >= iters - 1:\n",
    "                break\n",
    "    t_small = timeit.default_timer() - start\n",
    "\n",
    "    # Full eval timing\n",
    "    start = timeit.default_timer()\n",
    "    acc = evaluate(model, data, desc=f\"Eval runtime SA {r}x{c}\")\n",
    "    t_full = timeit.default_timer() - start\n",
    "\n",
    "    results.append({\n",
    "        \"sa_rows\": r,\n",
    "        \"sa_cols\": c,\n",
    "        \"accuracy %\": acc,\n",
    "        \"time_10_batches_sec\": t_small,\n",
    "        \"time_full_eval_sec\": t_full,\n",
    "    })\n",
    "\n",
    "df_rt = pd.DataFrame(results).sort_values(by=[\"sa_rows\",\"sa_cols\"]).reset_index(drop=True)\n",
    "df_rt\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
