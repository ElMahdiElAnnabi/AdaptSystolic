{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation with AdaPT on MNIST dataset\n",
    "\n",
    "In this notebook you can evaluate different approximate multipliers on various models based on MNIST dataset\n",
    "\n",
    "Steps:\n",
    "* Select number of threads to use\n",
    "* Load dataset\n",
    "* Load Adapt Layers\n",
    "* Define Model\n",
    "* Run model calibration for quantization\n",
    "* Evaluate\n",
    "\n",
    "\n",
    "**Note**:\n",
    "* This notebook should be run on a X86 machine\n",
    "\n",
    "* Please make sure you have run the installation steps first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select number of threads to use\n",
    "\n",
    "For optimal performance set them as the number of your cpu threads (not cpu cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OMP_PLACES=cores\n",
      "env: OMP_PROC_BIND=close\n",
      "env: OMP_WAIT_POLICY=active\n"
     ]
    }
   ],
   "source": [
    "threads = 40\n",
    "torch.set_num_threads(threads)\n",
    "\n",
    "#maybe better performance\n",
    "%env OMP_PLACES=cores\n",
    "%env OMP_PROC_BIND=close\n",
    "%env OMP_WAIT_POLICY=active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = MNIST('./datasets/mnist_data/data', train=True, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "]), )\n",
    "\n",
    "test = MNIST('./datasets/mnist_data/data', train=False, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "]), )\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_args = dict(shuffle=True, batch_size=64,num_workers=0 , pin_memory=False)\n",
    "train_loader = dataloader.DataLoader(train, **dataloader_args)\n",
    "test_loader = dataloader.DataLoader(test, **dataloader_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Adapt Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load ADAPT layers\n",
    "from adapt.approx_layers import axx_layers as approxNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose approximate multiplier \n",
    "\n",
    "Two approximate multipliers are already provided\n",
    "\n",
    "**mul8s_acc** - (header file: mul8s_acc.h)   <--  default\n",
    "\n",
    "**mul8s_1L2H** - (header file: mul8s_1L2H.h)\n",
    "\n",
    "\n",
    "\n",
    "In order to use your custom multiplier you need to use the provided tool (LUT_generator) to easily create the C++ header for your multiplier. Then you just place it inside the adapt/cpu-kernels/axx_mults folder. The name of the axx_mult here must match the name of the header file. The same axx_mult is used in all layers. \n",
    "\n",
    "Tip: If you want explicitly to set for each layer a different axx_mult you must do it from the model definition using the respective AdaPT_Conv2d class of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "axx_mult = 'mul8s_1L2H'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "Jit compilation method loads 'on the fly' the C++ extentions of the approximate multipliers. Then the pytorch model is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/PyInit_linear_mul8s_1L2H/build.ninja...\n",
      "Building extension module PyInit_linear_mul8s_1L2H...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module PyInit_linear_mul8s_1L2H...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_linear_mul8s_1L2H, skipping build step...\n",
      "Loading extension module PyInit_linear_mul8s_1L2H...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_linear_mul8s_1L2H, skipping build step...\n",
      "Loading extension module PyInit_linear_mul8s_1L2H...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set flag for use of AdaPT custom layers or vanilla PyTorch\n",
    "use_adapt=True\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        if use_adapt:\n",
    "             self.fc1 = approxNN.AdaPT_Linear(784, 548, axx_mult = axx_mult)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(784, 548)\n",
    "        \n",
    "        self.bc1 = nn.BatchNorm1d(548)\n",
    "\n",
    "        if use_adapt:\n",
    "            self.fc2 = approxNN.AdaPT_Linear(548, 252, axx_mult = axx_mult)\n",
    "        else:    \n",
    "            self.fc2 = nn.Linear(548, 252)\n",
    "            \n",
    "        self.bc2 = nn.BatchNorm1d(252)     \n",
    "        \n",
    "        if use_adapt:\n",
    "            self.fc3 = approxNN.AdaPT_Linear(252, 10, axx_mult = axx_mult)\n",
    "        else:\n",
    "            self.fc3 = nn.Linear(252, 10)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = x.view((-1, 784))\n",
    "        h = self.fc1(x)\n",
    "        h = self.bc1(h)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        \n",
    "        h = self.fc2(h)\n",
    "        h = self.bc2(h)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=0.2, training=self.training)\n",
    "        \n",
    "        h = self.fc3(h)\n",
    "        out = F.log_softmax(h,-1)\n",
    "        return out\n",
    "\n",
    "model =Model()\n",
    "model.cpu()\n",
    "\n",
    "#load pretrained weights\n",
    "model.load_state_dict(torch.load('models/state_dicts/mnist.pt'))\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model calibration for quantization\n",
    "\n",
    "Calibrates the quantization parameters \n",
    "\n",
    "Need to re-run it each time the model changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0822 16:33:04.832009 140603895383872 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0822 16:33:04.832573 140603895383872 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0822 16:33:04.832999 140603895383872 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0822 16:33:04.833375 140603895383872 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0822 16:33:04.833750 140603895383872 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0822 16:33:04.834131 140603895383872 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0822 16:33:04.835496 140603895383872 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0822 16:33:04.836484 140603895383872 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0822 16:33:04.837378 140603895383872 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0822 16:33:04.838256 140603895383872 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0822 16:33:04.839241 140603895383872 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0822 16:33:04.840203 140603895383872 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.quantizer                           : TensorQuantizer(8bit per-tensor amax=254.8755 calibrator=HistogramCalibrator scale=0.4982825219631195 quant)\n",
      "fc1.quantizer_w                         : TensorQuantizer(8bit per-tensor amax=0.5369 calibrator=HistogramCalibrator scale=236.53900146484375 quant)\n",
      "fc2.quantizer                           : TensorQuantizer(8bit per-tensor amax=5.3846 calibrator=HistogramCalibrator scale=22.013214111328125 quant)\n",
      "fc2.quantizer_w                         : TensorQuantizer(8bit per-tensor amax=0.4709 calibrator=HistogramCalibrator scale=269.6973571777344 quant)\n",
      "fc3.quantizer                           : TensorQuantizer(8bit per-tensor amax=5.8338 calibrator=HistogramCalibrator scale=20.485553741455078 quant)\n",
      "fc3.quantizer_w                         : TensorQuantizer(8bit per-tensor amax=0.5777 calibrator=HistogramCalibrator scale=219.846435546875 quant)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import calib\n",
    "\n",
    "def collect_stats(model, data_loader, num_batches):\n",
    "     \"\"\"Feed data to the network and collect statistic\"\"\"\n",
    "\n",
    "     # Enable calibrators\n",
    "     for name, module in model.named_modules():\n",
    "         if isinstance(module, quant_nn.TensorQuantizer):\n",
    "             if module._calibrator is not None:\n",
    "                 module.disable_quant()\n",
    "                 module.enable_calib()\n",
    "             else:\n",
    "                 module.disable()\n",
    "        \n",
    "     evaluate_x = Variable(data_loader.dataset.data.type_as(torch.FloatTensor())).cpu()\n",
    "     model(evaluate_x)\n",
    "        \n",
    "     # Disable calibrators\n",
    "     for name, module in model.named_modules():\n",
    "         if isinstance(module, quant_nn.TensorQuantizer):\n",
    "             if module._calibrator is not None:\n",
    "                 module.enable_quant()\n",
    "                 module.disable_calib()\n",
    "             else:\n",
    "                 module.enable()\n",
    "\n",
    "def compute_amax(model, **kwargs):\n",
    " # Load calib result\n",
    " for name, module in model.named_modules():\n",
    "     if isinstance(module, quant_nn.TensorQuantizer):\n",
    "         if module._calibrator is not None:\n",
    "             if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "                 module.load_calib_amax()\n",
    "             else:\n",
    "                 module.load_calib_amax(**kwargs)\n",
    "         print(F\"{name:40}: {module}\")\n",
    " model.cpu()\n",
    "\n",
    "# It is a bit slow since we collect histograms on CPU\n",
    "with torch.no_grad():\n",
    "    stats = collect_stats(model, test_loader, num_batches=2)\n",
    "    amax = compute_amax(model, method=\"percentile\", percentile=99.99)\n",
    "    \n",
    "    # optional - test different calibration methods\n",
    "    #amax = compute_amax(model, method=\"mse\")\n",
    "    #amax = compute_amax(model, method=\"entropy\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# force TensorQuantizers to load buffers on CPU instead of .cuda()\n",
    "def force_quantizer_cpu():\n",
    "    import torch\n",
    "    from pytorch_quantization.nn import modules\n",
    "\n",
    "    orig_fn = modules.tensor_quantizer.TensorQuantizer._load_from_state_dict\n",
    "\n",
    "    def new_fn(self, state_dict, prefix, *args, **kwargs):\n",
    "        key = prefix + '_amax'\n",
    "        if key in state_dict:\n",
    "            buf = state_dict[key].data.cpu()   # force CPU\n",
    "            self.register_buffer(\"_amax\", buf)\n",
    "        else:\n",
    "            # fallback to original\n",
    "            orig_fn(self, state_dict, prefix, *args, **kwargs)\n",
    "\n",
    "    modules.tensor_quantizer.TensorQuantizer._load_from_state_dict = new_fn\n",
    "\n",
    "force_quantizer_cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_linear_mul8s_1L2H, skipping build step...\n",
      "Loading extension module PyInit_linear_mul8s_1L2H...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_linear_mul8s_1L2H, skipping build step...\n",
      "Loading extension module PyInit_linear_mul8s_1L2H...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_linear_mul8s_1L2H, skipping build step...\n",
      "Loading extension module PyInit_linear_mul8s_1L2H...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_linear_mul8s_1L2H, skipping build step...\n",
      "Loading extension module PyInit_linear_mul8s_1L2H...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_linear_mul8s_1L2H, skipping build step...\n",
      "Loading extension module PyInit_linear_mul8s_1L2H...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_linear_mul8s_1L2H, skipping build step...\n",
      "Loading extension module PyInit_linear_mul8s_1L2H...\n",
      "Baseline (normal CPU): 0.9792\n",
      "Exact systolic:        0.9792\n",
      "Approx systolic:       0.9792\n",
      "Δ (Exact - Approx):    0.0000\n"
     ]
    }
   ],
   "source": [
    "from adapt.approx_layers.systolic_utils import compare_exact_vs_approx\n",
    "\n",
    "acc_baseline, acc_exact, acc_approx, delta = compare_exact_vs_approx(\n",
    "    model, test_loader, axx_mult=\"mul8s_acc\", device=\"cpu\"\n",
    ")\n",
    "\n",
    "print(f'Baseline (normal CPU): {acc_baseline:.4f}')\n",
    "print(f'Exact systolic:        {acc_exact:.4f}')\n",
    "print(f'Approx systolic:       {acc_approx:.4f}')\n",
    "print(f'Δ (Exact - Approx):    {delta:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: tensor(0.9788)\n"
     ]
    }
   ],
   "source": [
    "evaluate_x = Variable(test_loader.dataset.data.type_as(torch.FloatTensor())).cpu()\n",
    "evaluate_y = Variable(test_loader.dataset.targets).cpu()\n",
    "\n",
    "\n",
    "output = model(evaluate_x)\n",
    "pred = output.data.max(1)[1]\n",
    "d = pred.eq(evaluate_y.data).cpu()\n",
    "accuracy = d.sum()/d.size()[0]\n",
    "\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current path: /workspace/adapt/examples\n",
      "Parent path: /workspace/adapt\n",
      "ADaPT root: /workspace/adapt/adapt\n",
      "SRC_DIR: /workspace/adapt/adapt/cpu-kernels\n",
      "Linear candidates: ['/workspace/adapt/adapt/cpu-kernels/axx_linear_systolic.cpp', '/workspace/adapt/adapt/cpu-kernels/axx_linear.cpp']\n",
      "Conv2d src: /workspace/adapt/adapt/cpu-kernels/axx_conv2d_systolic.cpp\n",
      "[build] Trying to load: PyInit_linear_systolic_mul8s_acc from /workspace/adapt/adapt/cpu-kernels/axx_linear_systolic.cpp\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_linear_systolic_mul8s_acc_v2, skipping build step...\n",
      "Loading extension module PyInit_linear_systolic_mul8s_acc_v2...\n",
      "[warn] First attempt failed for PyInit_linear_systolic_mul8s_acc: No module named 'PyInit_linear_systolic_mul8s_acc_v2'\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "[clean] Removing build dir: /root/.cache/torch_extensions/PyInit_linear_systolic_mul8s_acc\n",
      "[build] Trying to load: PyInit_linear_systolic_mul8s_acc from /workspace/adapt/adapt/cpu-kernels/axx_linear_systolic.cpp\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/PyInit_linear_systolic_mul8s_acc...\n",
      "No modifications detected for re-loaded extension module PyInit_linear_systolic_mul8s_acc_v2, skipping build step...\n",
      "Loading extension module PyInit_linear_systolic_mul8s_acc_v2...\n",
      "[warn] Retry after clean failed for PyInit_linear_systolic_mul8s_acc: No module named 'PyInit_linear_systolic_mul8s_acc_v2'\n",
      "[build] Trying to load: PyInit_linear_systolic_mul8s_acc_t1755875873 from /workspace/adapt/adapt/cpu-kernels/axx_linear_systolic.cpp\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/PyInit_linear_systolic_mul8s_acc_t1755875873...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/PyInit_linear_systolic_mul8s_acc_t1755875873/build.ninja...\n",
      "Building extension module PyInit_linear_systolic_mul8s_acc_t1755875873...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/2] c++ -MMD -MF axx_linear_systolic.o.d -DTORCH_EXTENSION_NAME=PyInit_linear_systolic_mul8s_acc_t1755875873 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.8/dist-packages/torch/include/THC -isystem /usr/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -DAXX_MULT=mul8s_acc -O3 -fopenmp -march=native -std=c++17 -I/workspace/adapt/adapt -c /workspace/adapt/adapt/cpu-kernels/axx_linear_systolic.cpp -o axx_linear_systolic.o \n",
      "FAILED: axx_linear_systolic.o \n",
      "c++ -MMD -MF axx_linear_systolic.o.d -DTORCH_EXTENSION_NAME=PyInit_linear_systolic_mul8s_acc_t1755875873 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.8/dist-packages/torch/include/THC -isystem /usr/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -DAXX_MULT=mul8s_acc -O3 -fopenmp -march=native -std=c++17 -I/workspace/adapt/adapt -c /workspace/adapt/adapt/cpu-kernels/axx_linear_systolic.cpp -o axx_linear_systolic.o \n",
      "/workspace/adapt/adapt/cpu-kernels/axx_linear_systolic.cpp: In function ‘at::Tensor systolic_linear_forward(at::Tensor, at::Tensor)’:\n",
      "/workspace/adapt/adapt/cpu-kernels/axx_linear_systolic.cpp:34:51: error: use of deleted function ‘at::TensorAccessor<T, N> at::Tensor::accessor() && [with T = signed char; long unsigned int N = 2]’\n",
      "   34 |     auto Aacc = A.contiguous().accessor<int8_t,2>();\n",
      "      |                                                   ^\n",
      "In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
      "                 from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4,\n",
      "                 from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9,\n",
      "                 from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                 from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                 from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                 from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                 from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                 from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                 from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                 from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
      "                 from /workspace/adapt/adapt/cpu-kernels/axx_linear_systolic.cpp:1:\n",
      "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:417:23: note: declared here\n",
      "  417 |   TensorAccessor<T,N> accessor() && = delete;\n",
      "      |                       ^~~~~~~~\n",
      "/workspace/adapt/adapt/cpu-kernels/axx_linear_systolic.cpp:35:51: error: use of deleted function ‘at::TensorAccessor<T, N> at::Tensor::accessor() && [with T = signed char; long unsigned int N = 2]’\n",
      "   35 |     auto Bacc = B.contiguous().accessor<int8_t,2>();   // B is [N,K]\n",
      "      |                                                   ^\n",
      "In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
      "                 from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4,\n",
      "                 from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9,\n",
      "                 from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                 from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                 from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                 from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                 from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                 from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                 from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                 from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
      "                 from /workspace/adapt/adapt/cpu-kernels/axx_linear_systolic.cpp:1:\n",
      "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:417:23: note: declared here\n",
      "  417 |   TensorAccessor<T,N> accessor() && = delete;\n",
      "      |                       ^~~~~~~~\n",
      "ninja: build stopped: subcommand failed.\n",
      "[error] Could not load linear kernel at all. Falling back to Python.\n",
      "  Reason: Error building extension 'PyInit_linear_systolic_mul8s_acc_t1755875873'\n",
      "[info] Loaded C++ conv2d kernel: PyInit_conv2d_systolic_mul8s_acc from /workspace/adapt/adapt/cpu-kernels/axx_conv2d_systolic.cpp\n",
      "\n",
      "=== Linear correctness test ===\n",
      "Input A (M x K):\n",
      " tensor([[-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920],\n",
      "        [-0.3160, -2.1152,  0.4681, -0.1577,  1.4437,  0.2660],\n",
      "        [ 0.1665,  0.8744, -0.1435, -0.1116,  0.9318,  1.2590],\n",
      "        [ 2.0050,  0.0537,  0.6181, -0.4128, -0.8411, -2.3160]])\n",
      "Input B (N x K):\n",
      " tensor([[-0.2159, -0.7425,  0.5627,  0.2596, -0.1740, -0.6787],\n",
      "        [ 0.9383,  0.4889,  1.2032,  0.0845, -1.2001, -0.0048],\n",
      "        [-0.5181, -0.3067, -0.4731,  0.3356,  1.5091,  2.0820],\n",
      "        [ 1.7067,  2.3804, -1.1256, -0.3170, -0.1407,  0.8058],\n",
      "        [ 0.3276, -0.7607, -1.5991,  0.0185, -0.7504,  0.1854]])\n",
      "FP32 Reference Output (M x N):\n",
      " tensor([[ 0.2277, -2.9797,  3.6312, -3.8067,  0.3919],\n",
      "        [ 1.4296, -2.5145,  3.2706, -6.0399, -0.2799],\n",
      "        [-1.8116, -0.7228,  3.7035,  3.4457, -0.8491],\n",
      "        [ 1.4862,  3.6367, -7.5773,  1.2372, -0.1782]])\n",
      "Quantized A (int8):\n",
      " tensor([[ -62,  -63,  -14,  -24,   47,   38],\n",
      "        [ -17, -116,   26,   -9,   79,   15],\n",
      "        [   9,   48,   -8,   -6,   51,   69],\n",
      "        [ 110,    3,   34,  -23,  -46, -127]], dtype=torch.int8)\n",
      "Quantized B (int8):\n",
      " tensor([[-12, -40,  30,  14,  -9, -36],\n",
      "        [ 50,  26,  64,   5, -64,   0],\n",
      "        [-28, -16, -25,  18,  81, 111],\n",
      "        [ 91, 127, -60, -17,  -8,  43],\n",
      "        [ 17, -41, -85,   1, -40,  10]], dtype=torch.int8)\n",
      "Int32 Accumulator (Python) C:\n",
      " tensor([[   717,  -8762,  10687, -11137,   1195],\n",
      "        [  4247,  -7303,   9584, -17673,   -762],\n",
      "        [ -5295,  -2108,  10862,  10056,  -2491],\n",
      "        [  4244,  10583, -22215,   3649,   -596]], dtype=torch.int32)\n",
      "Dequantized INT8 Output (Python):\n",
      " tensor([[ 0.2451, -2.9949,  3.6529, -3.8067,  0.4085],\n",
      "        [ 1.4517, -2.4962,  3.2759, -6.0408, -0.2605],\n",
      "        [-1.8099, -0.7205,  3.7127,  3.4372, -0.8514],\n",
      "        [ 1.4506,  3.6174, -7.5933,  1.2473, -0.2037]])\n",
      "[info] C++ linear kernel not used; compared Python systolic to FP32.\n",
      "INT8-dequant vs FP32 | max abs diff: 0.035596\n",
      "\n",
      "=== Conv2d correctness test ===\n",
      "Input X (N x C x H x W):\n",
      " tensor([[[[ 1.0395,  0.3582, -0.0033, -0.5344,  1.1687,  0.3945,  1.9415],\n",
      "          [ 0.7915,  0.0335,  0.7101, -1.5353, -0.4127,  0.9663,  1.6248],\n",
      "          [-0.3656, -1.3024, -0.2282,  0.2800,  0.0732,  1.1133,  0.2823],\n",
      "          [ 0.4342, -0.8025, -1.2952,  0.7813, -0.9268,  0.2064, -0.3334],\n",
      "          [-0.4288,  0.2329,  0.7969, -0.1848, -0.9215, -0.0562, -0.7015],\n",
      "          [ 1.0367, -0.6037, -1.2788,  0.0930, -0.6661,  0.9234,  1.3873],\n",
      "          [ 1.3750,  0.6596,  0.4766, -1.0163,  0.1804,  0.1083,  1.9507]],\n",
      "\n",
      "         [[-1.0631,  1.1404, -0.0899,  0.7298, -1.8453, -0.0250,  1.3694],\n",
      "          [-0.3126,  0.2458,  0.3772,  1.1012, -1.1428,  0.0376,  2.6963],\n",
      "          [ 1.2358, -0.2011, -0.1179, -0.8294, -1.4073,  1.6268,  0.1723],\n",
      "          [-1.6115, -0.4794,  0.1574,  0.3854,  0.5737,  0.9979,  0.5436],\n",
      "          [ 0.0788,  0.8629, -0.0195,  0.7611,  0.6183,  0.9874, -1.4878],\n",
      "          [ 0.5867,  0.1583,  0.1102, -0.8188,  0.6328, -1.9169,  1.3119],\n",
      "          [-0.2098,  0.7817,  0.9897,  0.4147, -1.5090,  2.0360,  0.1316]]]])\n",
      "Weights W (O x C x Kh x Kw):\n",
      " tensor([[[[-0.5111, -1.7137, -0.5101],\n",
      "          [-0.4749, -0.6334, -1.4677],\n",
      "          [-0.8785, -2.0784, -1.1005]],\n",
      "\n",
      "         [[-0.7201,  0.0119,  0.3398],\n",
      "          [-0.2635,  1.2805,  0.0194],\n",
      "          [-0.8808,  0.4386, -0.0107]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3384, -0.2794, -0.5518],\n",
      "          [-2.8891, -1.5100,  1.0241],\n",
      "          [ 0.1954, -0.7371,  1.7001]],\n",
      "\n",
      "         [[ 0.3462,  0.9711,  1.4503],\n",
      "          [-0.0519, -0.6284, -0.6538],\n",
      "          [ 1.7198, -0.9610, -0.6375]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0747,  0.5600,  0.0625],\n",
      "          [ 0.6481, -0.0394, -0.8015],\n",
      "          [-0.4955, -0.3615,  0.5851]],\n",
      "\n",
      "         [[-1.1560, -0.6710, -0.5804],\n",
      "          [-0.0856,  1.3945,  0.5969],\n",
      "          [-0.4828, -0.3661, -1.3271]]]])\n",
      "FP32 Reference Output (N x O x Ho x Wo):\n",
      " tensor([[[[-4.3449, -0.1444,  0.3365,  2.7315, -3.9497, -5.6134, -2.7333],\n",
      "          [ 0.2021,  0.6743,  3.1929,  1.5906, -6.1950, -3.6067, -4.4619],\n",
      "          [ 1.7068,  4.1995,  3.0131,  1.1554, -2.2459,  1.4474, -4.3175],\n",
      "          [ 0.7276,  2.9446, -1.2685,  2.1825,  2.5918,  2.5408, -1.6908],\n",
      "          [-1.7042,  4.2951,  5.1312,  3.1648,  3.8222, -1.6239, -3.3711],\n",
      "          [-1.7903, -1.4086, -1.4200,  1.8950,  0.8581, -5.2239, -4.5153],\n",
      "          [-3.5005,  0.0790,  4.0076,  1.3536, -1.2236, -1.9925, -3.1082]],\n",
      "\n",
      "         [[-1.6629, -3.8271, -5.8257,  3.6537,  5.9366, -4.5186, -8.4658],\n",
      "          [-3.9958,  2.8012, -0.8841, -1.2541,  4.6318, -3.0445, -2.9930],\n",
      "          [-1.4439,  0.3274,  8.6678,  0.0389, -2.4001, -2.0524,  1.1153],\n",
      "          [ 1.6643, -0.6069,  1.6889, -4.7574, -0.6011,  1.7174,  5.2895],\n",
      "          [-4.1215,  1.1547, -1.5223, -4.9057,  5.4074,  6.8324, -2.0513],\n",
      "          [-1.5254, -4.9873,  3.9815,  9.6372,  5.6360, -0.9140, -4.5111],\n",
      "          [-0.9372, -2.8295, -6.1273, -0.1921, -0.2776, -1.6291, -1.9883]],\n",
      "\n",
      "         [[-1.6080,  1.8613, -2.0165, -0.1009, -2.0471, -2.1080,  0.0195],\n",
      "          [-0.5068,  1.4830,  2.8291,  4.4143, -3.1274,  1.5184,  3.0434],\n",
      "          [ 3.7737, -0.2975, -1.5885, -4.7095, -4.0993,  1.4379, -0.7218],\n",
      "          [-3.7887, -0.8153, -1.9507,  0.4359,  1.3181,  3.7880, -0.5860],\n",
      "          [ 0.8549,  0.3232,  1.8952,  1.8208,  2.9333, -1.7562, -4.3716],\n",
      "          [-0.4953, -0.6929, -4.1220, -0.4112, -5.0944, -3.3608,  0.2070],\n",
      "          [-0.3516,  0.9931,  2.2470, -0.2332, -0.3508,  1.9436,  2.1842]]]])\n",
      "Quantized Input X (int8):\n",
      " tensor([[[[ 49,  17,   0, -25,  55,  19,  91],\n",
      "          [ 37,   2,  33, -72, -19,  46,  77],\n",
      "          [-17, -61, -11,  13,   3,  52,  13],\n",
      "          [ 20, -38, -61,  37, -44,  10, -16],\n",
      "          [-20,  11,  38,  -9, -43,  -3, -33],\n",
      "          [ 49, -28, -60,   4, -31,  43,  65],\n",
      "          [ 65,  31,  22, -48,   8,   5,  92]],\n",
      "\n",
      "         [[-50,  54,  -4,  34, -87,  -1,  65],\n",
      "          [-15,  12,  18,  52, -54,   2, 127],\n",
      "          [ 58,  -9,  -6, -39, -66,  77,   8],\n",
      "          [-76, -23,   7,  18,  27,  47,  26],\n",
      "          [  4,  41,  -1,  36,  29,  47, -70],\n",
      "          [ 28,   7,   5, -39,  30, -90,  62],\n",
      "          [-10,  37,  47,  20, -71,  96,   6]]]], dtype=torch.int8)\n",
      "Quantized Weights W (int8):\n",
      " tensor([[[[ -22,  -75,  -22],\n",
      "          [ -21,  -28,  -65],\n",
      "          [ -39,  -91,  -48]],\n",
      "\n",
      "         [[ -32,    1,   15],\n",
      "          [ -12,   56,    1],\n",
      "          [ -39,   19,    0]]],\n",
      "\n",
      "\n",
      "        [[[  59,  -12,  -24],\n",
      "          [-127,  -66,   45],\n",
      "          [   9,  -32,   75]],\n",
      "\n",
      "         [[  15,   43,   64],\n",
      "          [  -2,  -28,  -29],\n",
      "          [  76,  -42,  -28]]],\n",
      "\n",
      "\n",
      "        [[[   3,   25,    3],\n",
      "          [  28,   -2,  -35],\n",
      "          [ -22,  -16,   26]],\n",
      "\n",
      "         [[ -51,  -29,  -26],\n",
      "          [  -4,   61,   26],\n",
      "          [ -21,  -16,  -58]]]], dtype=torch.int8)\n",
      "Int32 Accumulator (Python) Y:\n",
      " tensor([[[[ -8971,   -281,    679,   5453,  -8256, -11546,  -5761],\n",
      "          [   294,   1442,   6577,   3317, -12784,  -7322,  -9242],\n",
      "          [  3586,   8779,   6243,   2445,  -4687,   3071,  -8929],\n",
      "          [  1539,   6054,  -2708,   4544,   5385,   5371,  -3488],\n",
      "          [ -3558,   8713,  10570,   6568,   7941,  -3215,  -6899],\n",
      "          [ -3693,  -2926,  -2959,   3910,   1893, -10740,  -9401],\n",
      "          [ -7284,    104,   8285,   2800,  -2442,  -4147,  -6376]],\n",
      "\n",
      "         [[ -3375,  -8045, -12004,   7576,  12371,  -9439, -17469],\n",
      "          [ -8185,   5757,  -1853,  -2666,   9505,  -6280,  -6107],\n",
      "          [ -3009,    625,  17946,     -4,  -4945,  -4167,   2523],\n",
      "          [  3500,  -1164,   3535,  -9839,  -1240,   3582,  10916],\n",
      "          [ -8594,   2347,  -3316, -10209,  11058,  14132,  -4107],\n",
      "          [ -3080, -10324,   8211,  19998,  11674,  -1831,  -9248],\n",
      "          [ -1952,  -5982, -12723,   -387,   -522,  -3283,  -3994]],\n",
      "\n",
      "         [[ -3335,   3819,  -4225,   -193,  -4204,  -4286,      1],\n",
      "          [ -1145,   3135,   5769,   9139,  -6439,   3140,   6274],\n",
      "          [  7769,   -557,  -3255,  -9685,  -8407,   2977,  -1479],\n",
      "          [ -7836,  -1726,  -4039,    937,   2604,   7851,  -1241],\n",
      "          [  1787,    655,   3947,   3799,   5989,  -3560,  -9085],\n",
      "          [ -1097,  -1527,  -8554,   -906, -10489,  -6809,    321],\n",
      "          [  -716,   2013,   4683,   -459,   -625,   3935,   4484]]]],\n",
      "       dtype=torch.int32)\n",
      "Dequantized INT8 Output (Python):\n",
      " tensor([[[[-4.3327e+00, -1.3571e-01,  3.2793e-01,  2.6336e+00, -3.9873e+00,\n",
      "           -5.5763e+00, -2.7823e+00],\n",
      "          [ 1.4199e-01,  6.9643e-01,  3.1764e+00,  1.6020e+00, -6.1742e+00,\n",
      "           -3.5363e+00, -4.4635e+00],\n",
      "          [ 1.7319e+00,  4.2399e+00,  3.0151e+00,  1.1808e+00, -2.2636e+00,\n",
      "            1.4832e+00, -4.3124e+00],\n",
      "          [ 7.4328e-01,  2.9239e+00, -1.3079e+00,  2.1946e+00,  2.6008e+00,\n",
      "            2.5940e+00, -1.6846e+00],\n",
      "          [-1.7184e+00,  4.2081e+00,  5.1049e+00,  3.1721e+00,  3.8352e+00,\n",
      "           -1.5527e+00, -3.3320e+00],\n",
      "          [-1.7836e+00, -1.4131e+00, -1.4291e+00,  1.8884e+00,  9.1425e-01,\n",
      "           -5.1870e+00, -4.5403e+00],\n",
      "          [-3.5179e+00,  5.0228e-02,  4.0013e+00,  1.3523e+00, -1.1794e+00,\n",
      "           -2.0028e+00, -3.0794e+00]],\n",
      "\n",
      "         [[-1.6300e+00, -3.8854e+00, -5.7975e+00,  3.6589e+00,  5.9747e+00,\n",
      "           -4.5587e+00, -8.4369e+00],\n",
      "          [-3.9531e+00,  2.7804e+00, -8.9493e-01, -1.2876e+00,  4.5906e+00,\n",
      "           -3.0330e+00, -2.9495e+00],\n",
      "          [-1.4532e+00,  3.0185e-01,  8.6673e+00, -1.9319e-03, -2.3883e+00,\n",
      "           -2.0125e+00,  1.2185e+00],\n",
      "          [ 1.6904e+00, -5.6217e-01,  1.7073e+00, -4.7519e+00, -5.9887e-01,\n",
      "            1.7300e+00,  5.2720e+00],\n",
      "          [-4.1506e+00,  1.1335e+00, -1.6015e+00, -4.9306e+00,  5.3406e+00,\n",
      "            6.8252e+00, -1.9835e+00],\n",
      "          [-1.4875e+00, -4.9861e+00,  3.9656e+00,  9.6583e+00,  5.6381e+00,\n",
      "           -8.8430e-01, -4.4664e+00],\n",
      "          [-9.4274e-01, -2.8891e+00, -6.1447e+00, -1.8691e-01, -2.5211e-01,\n",
      "           -1.5856e+00, -1.9290e+00]],\n",
      "\n",
      "         [[-1.6107e+00,  1.8444e+00, -2.0405e+00, -9.3212e-02, -2.0304e+00,\n",
      "           -2.0700e+00,  4.8296e-04],\n",
      "          [-5.5299e-01,  1.5141e+00,  2.7862e+00,  4.4138e+00, -3.1098e+00,\n",
      "            1.5165e+00,  3.0301e+00],\n",
      "          [ 3.7521e+00, -2.6901e-01, -1.5720e+00, -4.6775e+00, -4.0603e+00,\n",
      "            1.4378e+00, -7.1430e-01],\n",
      "          [-3.7845e+00, -8.3359e-01, -1.9507e+00,  4.5254e-01,  1.2576e+00,\n",
      "            3.7917e+00, -5.9936e-01],\n",
      "          [ 8.6305e-01,  3.1634e-01,  1.9063e+00,  1.8348e+00,  2.8925e+00,\n",
      "           -1.7193e+00, -4.3877e+00],\n",
      "          [-5.2981e-01, -7.3748e-01, -4.1313e+00, -4.3756e-01, -5.0658e+00,\n",
      "           -3.2885e+00,  1.5503e-01],\n",
      "          [-3.4580e-01,  9.7220e-01,  2.2617e+00, -2.2168e-01, -3.0185e-01,\n",
      "            1.9005e+00,  2.1656e+00]]]])\n",
      "C++ Kernel Int32 Output:\n",
      " tensor([[[[ -8971,   -281,    679,   5453,  -8256, -11546,  -5761],\n",
      "          [   294,   1442,   6577,   3317, -12784,  -7322,  -9242],\n",
      "          [  3586,   8779,   6243,   2445,  -4687,   3071,  -8929],\n",
      "          [  1539,   6054,  -2708,   4544,   5385,   5371,  -3488],\n",
      "          [ -3558,   8713,  10570,   6568,   7941,  -3215,  -6899],\n",
      "          [ -3693,  -2926,  -2959,   3910,   1893, -10740,  -9401],\n",
      "          [ -7284,    104,   8285,   2800,  -2442,  -4147,  -6376]],\n",
      "\n",
      "         [[ -3375,  -8045, -12004,   7576,  12371,  -9439, -17469],\n",
      "          [ -8185,   5757,  -1853,  -2666,   9505,  -6280,  -6107],\n",
      "          [ -3009,    625,  17946,     -4,  -4945,  -4167,   2523],\n",
      "          [  3500,  -1164,   3535,  -9839,  -1240,   3582,  10916],\n",
      "          [ -8594,   2347,  -3316, -10209,  11058,  14132,  -4107],\n",
      "          [ -3080, -10324,   8211,  19998,  11674,  -1831,  -9248],\n",
      "          [ -1952,  -5982, -12723,   -387,   -522,  -3283,  -3994]],\n",
      "\n",
      "         [[ -3335,   3819,  -4225,   -193,  -4204,  -4286,      1],\n",
      "          [ -1145,   3135,   5769,   9139,  -6439,   3140,   6274],\n",
      "          [  7769,   -557,  -3255,  -9685,  -8407,   2977,  -1479],\n",
      "          [ -7836,  -1726,  -4039,    937,   2604,   7851,  -1241],\n",
      "          [  1787,    655,   3947,   3799,   5989,  -3560,  -9085],\n",
      "          [ -1097,  -1527,  -8554,   -906, -10489,  -6809,    321],\n",
      "          [  -716,   2013,   4683,   -459,   -625,   3935,   4484]]]],\n",
      "       dtype=torch.int32)\n",
      "C++ Kernel Dequantized Output:\n",
      " tensor([[[[-4.3327e+00, -1.3571e-01,  3.2793e-01,  2.6336e+00, -3.9873e+00,\n",
      "           -5.5763e+00, -2.7823e+00],\n",
      "          [ 1.4199e-01,  6.9643e-01,  3.1764e+00,  1.6020e+00, -6.1742e+00,\n",
      "           -3.5363e+00, -4.4635e+00],\n",
      "          [ 1.7319e+00,  4.2399e+00,  3.0151e+00,  1.1808e+00, -2.2636e+00,\n",
      "            1.4832e+00, -4.3124e+00],\n",
      "          [ 7.4328e-01,  2.9239e+00, -1.3079e+00,  2.1946e+00,  2.6008e+00,\n",
      "            2.5940e+00, -1.6846e+00],\n",
      "          [-1.7184e+00,  4.2081e+00,  5.1049e+00,  3.1721e+00,  3.8352e+00,\n",
      "           -1.5527e+00, -3.3320e+00],\n",
      "          [-1.7836e+00, -1.4131e+00, -1.4291e+00,  1.8884e+00,  9.1425e-01,\n",
      "           -5.1870e+00, -4.5403e+00],\n",
      "          [-3.5179e+00,  5.0228e-02,  4.0013e+00,  1.3523e+00, -1.1794e+00,\n",
      "           -2.0028e+00, -3.0794e+00]],\n",
      "\n",
      "         [[-1.6300e+00, -3.8854e+00, -5.7975e+00,  3.6589e+00,  5.9747e+00,\n",
      "           -4.5587e+00, -8.4369e+00],\n",
      "          [-3.9531e+00,  2.7804e+00, -8.9493e-01, -1.2876e+00,  4.5906e+00,\n",
      "           -3.0330e+00, -2.9495e+00],\n",
      "          [-1.4532e+00,  3.0185e-01,  8.6673e+00, -1.9319e-03, -2.3883e+00,\n",
      "           -2.0125e+00,  1.2185e+00],\n",
      "          [ 1.6904e+00, -5.6217e-01,  1.7073e+00, -4.7519e+00, -5.9887e-01,\n",
      "            1.7300e+00,  5.2720e+00],\n",
      "          [-4.1506e+00,  1.1335e+00, -1.6015e+00, -4.9306e+00,  5.3406e+00,\n",
      "            6.8252e+00, -1.9835e+00],\n",
      "          [-1.4875e+00, -4.9861e+00,  3.9656e+00,  9.6583e+00,  5.6381e+00,\n",
      "           -8.8430e-01, -4.4664e+00],\n",
      "          [-9.4274e-01, -2.8891e+00, -6.1447e+00, -1.8691e-01, -2.5211e-01,\n",
      "           -1.5856e+00, -1.9290e+00]],\n",
      "\n",
      "         [[-1.6107e+00,  1.8444e+00, -2.0405e+00, -9.3212e-02, -2.0304e+00,\n",
      "           -2.0700e+00,  4.8296e-04],\n",
      "          [-5.5299e-01,  1.5141e+00,  2.7862e+00,  4.4138e+00, -3.1098e+00,\n",
      "            1.5165e+00,  3.0301e+00],\n",
      "          [ 3.7521e+00, -2.6901e-01, -1.5720e+00, -4.6775e+00, -4.0603e+00,\n",
      "            1.4378e+00, -7.1430e-01],\n",
      "          [-3.7845e+00, -8.3359e-01, -1.9507e+00,  4.5254e-01,  1.2576e+00,\n",
      "            3.7917e+00, -5.9936e-01],\n",
      "          [ 8.6305e-01,  3.1634e-01,  1.9063e+00,  1.8348e+00,  2.8925e+00,\n",
      "           -1.7193e+00, -4.3877e+00],\n",
      "          [-5.2981e-01, -7.3748e-01, -4.1313e+00, -4.3756e-01, -5.0658e+00,\n",
      "           -3.2885e+00,  1.5503e-01],\n",
      "          [-3.4580e-01,  9.7220e-01,  2.2617e+00, -2.2168e-01, -3.0185e-01,\n",
      "            1.9005e+00,  2.1656e+00]]]])\n",
      "CPP vs FP32 | max abs diff: 0.103170\n",
      "CPP vs INT8-dequant | max abs diff: 0.000000\n",
      "INT8-dequant vs FP32 | max abs diff: 0.103170\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os, time, shutil, importlib\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.cpp_extension import load, _get_build_directory\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "torch.manual_seed(0)\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "# Multiplier selection (matches header: adapt/axx_mults/<AXX_MULT>.h)\n",
    "AXX_MULT = \"mul8s_acc\"    # e.g., \"mul8s_acc\"\n",
    "USE_EXACT = False         # True -> exact multiply; False -> LUT\n",
    "\n",
    "# =========================\n",
    "# Resolve paths\n",
    "# =========================\n",
    "cwd = os.getcwd()\n",
    "parent = os.path.abspath(os.path.join(cwd, \"..\"))          # one directory above the notebook\n",
    "adapt_root = os.path.join(parent, \"adapt\")                 # /workspace/adapt/adapt\n",
    "src_dir = os.path.join(adapt_root, \"cpu-kernels\")          # /workspace/adapt/adapt/cpu-kernels\n",
    "include_root = adapt_root                                  # so #include \"axx_mults/<AXX_MULT>.h\" works\n",
    "\n",
    "linear_src_candidates = [\n",
    "    os.path.join(src_dir, \"axx_linear_systolic.cpp\"),\n",
    "    os.path.join(src_dir, \"axx_linear.cpp\"),  # fallback if you only have the non-systolic file\n",
    "]\n",
    "conv2d_src = os.path.join(src_dir, \"axx_conv2d_systolic.cpp\")\n",
    "\n",
    "print(\"Current path:\", cwd)\n",
    "print(\"Parent path:\", parent)\n",
    "print(\"ADaPT root:\", adapt_root)\n",
    "print(\"SRC_DIR:\", src_dir)\n",
    "print(\"Linear candidates:\", linear_src_candidates)\n",
    "print(\"Conv2d src:\", conv2d_src)\n",
    "\n",
    "# =========================\n",
    "# Try load C++ extensions\n",
    "# =========================\n",
    "cpp_linear = None\n",
    "cpp_conv2d = None\n",
    "USE_CPP_LINEAR = True\n",
    "USE_CPP_CONV2D = True\n",
    "\n",
    "\n",
    "def _base_mod_name_for_src(src_path: str) -> str:\n",
    "    base = os.path.basename(src_path)\n",
    "    if base == \"axx_linear_systolic.cpp\":\n",
    "        return \"PyInit_linear_systolic_\"\n",
    "    elif base == \"axx_linear.cpp\":\n",
    "        return \"PyInit_linear_\"\n",
    "    return \"PyInit_linear_\"\n",
    "\n",
    "def _build_mod_name(src_path: str, exact: bool, axx_mult: str, tag: str = \"\") -> str:\n",
    "    base = _base_mod_name_for_src(src_path)\n",
    "    if exact:\n",
    "        base += \"exact_\"\n",
    "    name = base + axx_mult\n",
    "    if tag:\n",
    "        name += \"_\" + tag\n",
    "    return name\n",
    "\n",
    "def _clean_build_dir(mod_name: str):\n",
    "    try:\n",
    "        bdir = _get_build_directory(mod_name, verbose=True)\n",
    "        if os.path.isdir(bdir):\n",
    "            print(f\"[clean] Removing build dir: {bdir}\")\n",
    "            shutil.rmtree(bdir)\n",
    "    except Exception as e:\n",
    "        print(f\"[clean] Could not locate/remove build dir for {mod_name}: {e}\")\n",
    "\n",
    "def try_load_linear_kernel():\n",
    "    global cpp_linear, USE_CPP_LINEAR\n",
    "\n",
    "    src = next((p for p in linear_src_candidates if os.path.isfile(p)), None)\n",
    "    if src is None:\n",
    "        print(\"[warn] No linear .cpp found; falling back to Python.\")\n",
    "        USE_CPP_LINEAR = False\n",
    "        return\n",
    "\n",
    "    # 1) First attempt: deterministic name\n",
    "    mod_name = _build_mod_name(src, USE_EXACT, AXX_MULT)\n",
    "    cflags = [\n",
    "        f\"-DAXX_MULT={AXX_MULT}\",\n",
    "        \"-O3\", \"-fopenmp\", \"-march=native\", \"-std=c++17\",\n",
    "        f\"-I{include_root}\",\n",
    "    ]\n",
    "    if USE_EXACT:\n",
    "        cflags.append(\"-DUSE_EXACT\")\n",
    "\n",
    "    def _try(name):\n",
    "        print(f\"[build] Trying to load: {name} from {src}\")\n",
    "        return load(\n",
    "            name=name,\n",
    "            sources=[src],\n",
    "            extra_cflags=cflags,\n",
    "            extra_ldflags=[\"-lgomp\"],\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        cpp_linear = _try(mod_name)\n",
    "        print(f\"[info] Loaded C++ linear kernel: {mod_name}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] First attempt failed for {mod_name}: {e}\")\n",
    "\n",
    "    # 2) Clean cache for that name and retry same name once\n",
    "    _clean_build_dir(mod_name)\n",
    "    try:\n",
    "        cpp_linear = _try(mod_name)\n",
    "        print(f\"[info] Loaded C++ linear kernel after clean: {mod_name}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] Retry after clean failed for {mod_name}: {e}\")\n",
    "\n",
    "    # 3) Force a fresh unique module name (timestamp) to bypass any caching issues\n",
    "    unique_tag = f\"t{int(time.time())}\"\n",
    "    mod_name2 = _build_mod_name(src, USE_EXACT, AXX_MULT, tag=unique_tag)\n",
    "    try:\n",
    "        cpp_linear = _try(mod_name2)\n",
    "        print(f\"[info] Loaded C++ linear kernel with unique name: {mod_name2}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"[error] Could not load linear kernel at all. Falling back to Python.\\n  Reason: {e}\")\n",
    "        USE_CPP_LINEAR = False\n",
    "        cpp_linear = None\n",
    "def try_load_conv2d_kernel():\n",
    "    global cpp_conv2d, USE_CPP_CONV2D\n",
    "    if not os.path.isfile(conv2d_src):\n",
    "        print(f\"[warn] Conv2d source not found: {conv2d_src}; falling back to Python.\")\n",
    "        USE_CPP_CONV2D = False\n",
    "        return\n",
    "    mod_name = (\"PyInit_conv2d_systolic_exact_\" if USE_EXACT else \"PyInit_conv2d_systolic_\") + AXX_MULT\n",
    "    cflags = [f\"-DAXX_MULT={AXX_MULT}\", \"-O3\", \"-fopenmp\", \"-march=native\", f\"-I{include_root}\", \"-std=c++17\"]\n",
    "    if USE_EXACT:\n",
    "        cflags.append(\"-DUSE_EXACT\")\n",
    "    try:\n",
    "        cpp_conv2d = load(\n",
    "            name=mod_name,\n",
    "            sources=[conv2d_src],\n",
    "            extra_cflags=cflags,\n",
    "            extra_ldflags=[\"-lgomp\"],\n",
    "            verbose=False,\n",
    "        )\n",
    "        print(f\"[info] Loaded C++ conv2d kernel: {mod_name} from {conv2d_src}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] Could not load conv2d kernel. Falling back to Python.\\n  Reason: {e}\")\n",
    "        USE_CPP_CONV2D = False\n",
    "        cpp_conv2d = None\n",
    "\n",
    "try_load_linear_kernel()\n",
    "try_load_conv2d_kernel()\n",
    "\n",
    "# =========================\n",
    "# Quantization helpers\n",
    "# =========================\n",
    "def symmetric_int8_quant(x: torch.Tensor, amax: float):\n",
    "    max_value = 127.0\n",
    "    scale = max_value / max(1e-12, amax)\n",
    "    q = torch.clamp((x * scale).round(), -128, 127).to(torch.int8)\n",
    "    return q, scale\n",
    "\n",
    "def dequant_from_int32(acc_i32: torch.Tensor, scale_in: float, scale_w: float):\n",
    "    return acc_i32.to(torch.float32) / (scale_in * scale_w)\n",
    "\n",
    "# =========================\n",
    "# Python systolic simulators\n",
    "# =========================\n",
    "def systolic_linear_python(A_i8: torch.Tensor, B_i8: torch.Tensor, collect=False):\n",
    "    M, K = A_i8.shape\n",
    "    N = B_i8.shape[0]\n",
    "    C = torch.zeros((M, N), dtype=torch.int32)\n",
    "    snaps = []\n",
    "    for t in range(K):\n",
    "        a_col = A_i8[:, t].to(torch.int16)\n",
    "        b_col = B_i8[:, t].to(torch.int16)\n",
    "        C += (a_col.view(M, 1) * b_col.view(1, N)).to(torch.int32)\n",
    "        if collect:\n",
    "            snaps.append(C.clone())\n",
    "    return C, snaps\n",
    "\n",
    "def systolic_conv2d_python(X_i8: torch.Tensor, W_i8: torch.Tensor,\n",
    "                           kernel_size=(3,3), stride=(1,1), padding=(0,0),\n",
    "                           collect=False):\n",
    "    N, C, H, W = X_i8.shape\n",
    "    O, Cw, Kh, Kw = W_i8.shape\n",
    "    assert C == Cw\n",
    "    Sh, Sw = stride\n",
    "    Ph, Pw = padding\n",
    "    Ho = (H + 2*Ph - Kh)//Sh + 1\n",
    "    Wo = (W + 2*Pw - Kw)//Sw + 1\n",
    "\n",
    "    Xf = X_i8.to(torch.float32)  # unfold sometimes expects float\n",
    "    cols = torch.nn.functional.unfold(\n",
    "        Xf, kernel_size=(Kh, Kw), stride=(Sh, Sw), padding=(Ph, Pw)\n",
    "    ).to(torch.int8)  # [N, C*Kh*Kw, Ho*Wo]\n",
    "\n",
    "    K = C*Kh*Kw\n",
    "    L = Ho*Wo\n",
    "    W_flat = W_i8.view(O, K)\n",
    "\n",
    "    Y = torch.zeros((N, O, L), dtype=torch.int32)\n",
    "    snaps = []\n",
    "    for n in range(N):\n",
    "        for t in range(K):\n",
    "            x_t = cols[n, t, :]\n",
    "            w_t = W_flat[:, t]\n",
    "            Y[n] += (w_t.view(O, 1).to(torch.int16) * x_t.view(1, L).to(torch.int16)).to(torch.int32)\n",
    "        if collect:\n",
    "            snaps.append(Y[n].clone())\n",
    "\n",
    "    return Y.view(N, O, Ho, Wo), snaps\n",
    "\n",
    "# =========================\n",
    "# References using int8->int32->dequant\n",
    "# =========================\n",
    "def reference_linear_int8(A_f: torch.Tensor, B_f: torch.Tensor):\n",
    "    amax_x = A_f.abs().max().item()\n",
    "    amax_w = B_f.abs().max().item()\n",
    "    A_i8, s_x = symmetric_int8_quant(A_f, amax_x)\n",
    "    B_i8, s_w = symmetric_int8_quant(B_f, amax_w)\n",
    "    C_i32, snaps = systolic_linear_python(A_i8, B_i8, collect=True)\n",
    "    C_f = dequant_from_int32(C_i32, s_x, s_w)\n",
    "    return C_f, (A_i8, B_i8, C_i32, snaps), (s_x, s_w)\n",
    "\n",
    "def reference_conv2d_int8(X_f: torch.Tensor, W_f: torch.Tensor, stride=(1,1), padding=(0,0)):\n",
    "    amax_x = X_f.abs().max().item()\n",
    "    amax_w = W_f.abs().max().item()\n",
    "    X_i8, s_x = symmetric_int8_quant(X_f, amax_x)\n",
    "    W_i8, s_w = symmetric_int8_quant(W_f, amax_w)\n",
    "    Y_i32, snaps = systolic_conv2d_python(X_i8, W_i8, kernel_size=W_f.shape[-2:], stride=stride, padding=padding, collect=True)\n",
    "    Y_f = dequant_from_int32(Y_i32, s_x, s_w)\n",
    "    return Y_f, (X_i8, W_i8, Y_i32, snaps), (s_x, s_w)\n",
    "\n",
    "# =========================\n",
    "# Tests (print inputs/outputs)\n",
    "# =========================\n",
    "def test_linear(M=4, K=6, N=5):\n",
    "    print(\"\\n=== Linear correctness test ===\")\n",
    "    A = torch.randn(M, K)\n",
    "    B = torch.randn(N, K)  # [N,K]\n",
    "    print(\"Input A (M x K):\\n\", A)\n",
    "    print(\"Input B (N x K):\\n\", B)\n",
    "\n",
    "    # FP32 reference\n",
    "    ref_fp32 = A @ B.t()\n",
    "    print(\"FP32 Reference Output (M x N):\\n\", ref_fp32)\n",
    "\n",
    "    # Int8 systolic reference (python)\n",
    "    ref_int8, (A_i8, B_i8, C_i32_py, snaps), (s_x, s_w) = reference_linear_int8(A, B)\n",
    "    print(\"Quantized A (int8):\\n\", A_i8)\n",
    "    print(\"Quantized B (int8):\\n\", B_i8)\n",
    "    print(\"Int32 Accumulator (Python) C:\\n\", C_i32_py)\n",
    "    print(\"Dequantized INT8 Output (Python):\\n\", ref_int8)\n",
    "\n",
    "    # Optional: C++ kernel\n",
    "    if USE_CPP_LINEAR and cpp_linear is not None:\n",
    "        C_i32_cpp = cpp_linear.forward(A_i8, B_i8)\n",
    "        C_cpp_f = dequant_from_int32(C_i32_cpp, s_x, s_w)\n",
    "        print(\"C++ Kernel Int32 Output:\\n\", C_i32_cpp)\n",
    "        print(\"C++ Kernel Dequantized Output:\\n\", C_cpp_f)\n",
    "        diff_cpp = (C_cpp_f - ref_fp32).abs().max().item()\n",
    "        diff_cpp_int8 = (C_cpp_f - ref_int8).abs().max().item()\n",
    "        print(f\"CPP vs FP32 | max abs diff: {diff_cpp:.6f}\")\n",
    "        print(f\"CPP vs INT8-dequant | max abs diff: {diff_cpp_int8:.6f}\")\n",
    "    else:\n",
    "        print(\"[info] C++ linear kernel not used; compared Python systolic to FP32.\")\n",
    "\n",
    "    diff_int8 = (ref_int8 - ref_fp32).abs().max().item()\n",
    "    print(f\"INT8-dequant vs FP32 | max abs diff: {diff_int8:.6f}\")\n",
    "\n",
    "def test_conv(N=1, C=2, H=7, W=7, O=3, Kh=3, Kw=3, stride=(1,1), padding=(1,1)):\n",
    "    print(\"\\n=== Conv2d correctness test ===\")\n",
    "    X = torch.randn(N, C, H, W)\n",
    "    Wt = torch.randn(O, C, Kh, Kw)\n",
    "    print(\"Input X (N x C x H x W):\\n\", X)\n",
    "    print(\"Weights W (O x C x Kh x Kw):\\n\", Wt)\n",
    "\n",
    "    # FP32 reference\n",
    "    ref_fp32 = F.conv2d(X, Wt, bias=None, stride=stride, padding=padding)\n",
    "    print(\"FP32 Reference Output (N x O x Ho x Wo):\\n\", ref_fp32)\n",
    "\n",
    "    # Int8 systolic reference (python)\n",
    "    ref_int8, (X_i8, W_i8, Y_i32_py, snaps), (s_x, s_w) = reference_conv2d_int8(X, Wt, stride=stride, padding=padding)\n",
    "    print(\"Quantized Input X (int8):\\n\", X_i8)\n",
    "    print(\"Quantized Weights W (int8):\\n\", W_i8)\n",
    "    print(\"Int32 Accumulator (Python) Y:\\n\", Y_i32_py)\n",
    "    print(\"Dequantized INT8 Output (Python):\\n\", ref_int8)\n",
    "\n",
    "    # Optional: C++ kernel\n",
    "    if USE_CPP_CONV2D and cpp_conv2d is not None:\n",
    "        C_i32_cpp = cpp_conv2d.forward(X_i8, W_i8, [Kh, Kw], list(stride), list(padding))\n",
    "        C_cpp_f = dequant_from_int32(C_i32_cpp, s_x, s_w)\n",
    "        print(\"C++ Kernel Int32 Output:\\n\", C_i32_cpp)\n",
    "        print(\"C++ Kernel Dequantized Output:\\n\", C_cpp_f)\n",
    "        diff_cpp = (C_cpp_f - ref_fp32).abs().max().item()\n",
    "        diff_cpp_int8 = (C_cpp_f - ref_int8).abs().max().item()\n",
    "        print(f\"CPP vs FP32 | max abs diff: {diff_cpp:.6f}\")\n",
    "        print(f\"CPP vs INT8-dequant | max abs diff: {diff_cpp_int8:.6f}\")\n",
    "    else:\n",
    "        print(\"[info] C++ conv2d kernel not used; compared Python systolic to FP32.\")\n",
    "\n",
    "    diff_int8 = (ref_int8 - ref_fp32).abs().max().item()\n",
    "    print(f\"INT8-dequant vs FP32 | max abs diff: {diff_int8:.6f}\")\n",
    "\n",
    "# =========================\n",
    "# Run\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    test_linear(M=4, K=6, N=5)\n",
    "    test_conv(N=1, C=2, H=7, W=7, O=3, Kh=3, Kw=3, stride=(1,1), padding=(1,1))\n",
    "    print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------- Quant helpers for PyTorch quantized path ----------\n",
    "# ---------- Quant helpers for PyTorch quantized path ----------\n",
    "def quantize_activation_pt(x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    PyTorch-style per-tensor affine activation quantization (quint8).\n",
    "    scale = (xmax - xmin)/255, zp = round(-xmin/scale) clamped to [0,255]\n",
    "    \"\"\"\n",
    "    xmin = x.min().item()\n",
    "    xmax = x.max().item()\n",
    "    if xmax == xmin:\n",
    "        xmax = xmin + 1e-6\n",
    "    scale = (xmax - xmin) / 255.0\n",
    "    zp = int(round(-xmin / scale))\n",
    "    zp = max(0, min(255, zp))\n",
    "    xq = torch.quantize_per_tensor(x, scale=scale, zero_point=zp, dtype=torch.quint8)\n",
    "    return xq, scale, zp\n",
    "\n",
    "def quantize_weight_pt(w: torch.Tensor):\n",
    "    \"\"\"\n",
    "    PyTorch-style symmetric weight quantization (qint8): zero_point = 0\n",
    "    scale = max(|w|)/127\n",
    "    \"\"\"\n",
    "    amax = w.abs().max().item()\n",
    "    if amax < 1e-12:\n",
    "        amax = 1e-12\n",
    "    scale = amax / 127.0\n",
    "    wq = torch.quantize_per_tensor(w, scale=scale, zero_point=0, dtype=torch.qint8)\n",
    "    return wq, scale\n",
    "\n",
    "\n",
    "# ---------- Compare Linear ----------\n",
    "def compare_linear_systolic_vs_pytorch_int8(M=4, K=6, N=5):\n",
    "    print(\"\\n=== Compare Linear: Systolic INT8 vs PyTorch INT8 ===\")\n",
    "    torch.manual_seed(0)\n",
    "    A = torch.randn(M, K)          # activations\n",
    "    B = torch.randn(N, K)          # weights [out, in]\n",
    "\n",
    "    # FP32 reference\n",
    "    ref_fp32 = A @ B.t()\n",
    "\n",
    "    # -- PyTorch INT8 path --\n",
    "    Aq, s_x_pt, zp_x_pt = quantize_activation_pt(A)   # quint8\n",
    "    Wq, s_w_pt = quantize_weight_pt(B)                # qint8 (zp=0)\n",
    "\n",
    "    # Quantized Linear. Note: kwarg is bias_, not bias.\n",
    "    qlin = nn.quantized.Linear(in_features=K, out_features=N, bias_=False, dtype=torch.qint8)\n",
    "    qlin.set_weight_bias(Wq, None)\n",
    "    # The int32 accumulator scale is s_x_pt * s_w_pt; set module output quant params accordingly\n",
    "    qlin.scale = s_x_pt * s_w_pt\n",
    "    qlin.zero_point = 0\n",
    "\n",
    "    out_q = qlin(Aq)          # quantized output tensor\n",
    "    out_pt = out_q.dequantize()\n",
    "\n",
    "    # -- Your systolic INT8 path (symmetric int8) --\n",
    "    A_i8, s_x_sys = symmetric_int8_quant(A, A.abs().max().item())\n",
    "    B_i8, s_w_sys = symmetric_int8_quant(B, B.abs().max().item())\n",
    "    if 'cpp_linear' in globals() and cpp_linear is not None:\n",
    "        C_i32_sys = cpp_linear.forward(A_i8, B_i8)\n",
    "    else:\n",
    "        C_i32_sys, _ = systolic_linear_python(A_i8, B_i8, collect=False)\n",
    "    out_sys = dequant_from_int32(C_i32_sys, s_x_sys, s_w_sys)\n",
    "\n",
    "    # -- Metrics --\n",
    "    e_sys_vs_fp32 = (out_sys - ref_fp32).abs()\n",
    "    e_pt_vs_fp32  = (out_pt  - ref_fp32).abs()\n",
    "    e_sys_vs_pt   = (out_sys - out_pt ).abs()\n",
    "\n",
    "    print(f\"FP32 out shape: {ref_fp32.shape}\")\n",
    "    print(f\"Sys vs FP32:  max={e_sys_vs_fp32.max().item():.6f}, mean={e_sys_vs_fp32.mean().item():.6f}\")\n",
    "    print(f\"PT  vs FP32:  max={e_pt_vs_fp32.max().item():.6f}, mean={e_pt_vs_fp32.mean().item():.6f}\")\n",
    "    print(f\"Sys vs PT :   max={e_sys_vs_pt.max().item():.6f},  mean={e_sys_vs_pt.mean().item():.6f}\")\n",
    "\n",
    "\n",
    "# ---------- Compare Conv2d ----------\n",
    "def compare_conv_systolic_vs_pytorch_int8(N=1, C=2, H=7, W=7, O=3, Kh=3, Kw=3, stride=(1,1), padding=(1,1)):\n",
    "    print(\"\\n=== Compare Conv2d: Systolic INT8 vs PyTorch INT8 ===\")\n",
    "    torch.manual_seed(1)\n",
    "    X = torch.randn(N, C, H, W)\n",
    "    Wt = torch.randn(O, C, Kh, Kw)\n",
    "\n",
    "    # FP32 reference\n",
    "    ref_fp32 = F.conv2d(X, Wt, bias=None, stride=stride, padding=padding)\n",
    "\n",
    "    # -- PyTorch INT8 path via quantized op --\n",
    "    Xq, s_x_pt, zp_x_pt = quantize_activation_pt(X)  # quint8 input\n",
    "    Wq, s_w_pt = quantize_weight_pt(Wt)              # qint8 weight (zp=0)\n",
    "    out_scale = s_x_pt * s_w_pt\n",
    "    out_zp = 0\n",
    "\n",
    "    # torch.ops.quantized.conv2d(input, weight, bias, stride, padding, dilation, groups, out_scale, out_zero_point)\n",
    "    out_q = torch.ops.quantized.conv2d(\n",
    "        Xq, Wq, None, list(stride), list(padding), [1, 1], 1, out_scale, out_zp\n",
    "    )\n",
    "    out_pt = out_q.dequantize()\n",
    "\n",
    "    # -- Your systolic INT8 path (symmetric int8) --\n",
    "    X_i8, s_x_sys = symmetric_int8_quant(X, X.abs().max().item())\n",
    "    W_i8, s_w_sys = symmetric_int8_quant(Wt, Wt.abs().max().item())\n",
    "    if 'cpp_conv2d' in globals() and cpp_conv2d is not None:\n",
    "        Y_i32_sys = cpp_conv2d.forward(X_i8, W_i8, [Kh, Kw], list(stride), list(padding))\n",
    "    else:\n",
    "        Y_i32_sys, _ = systolic_conv2d_python(X_i8, W_i8, kernel_size=(Kh, Kw), stride=stride, padding=padding, collect=False)\n",
    "    out_sys = dequant_from_int32(Y_i32_sys, s_x_sys, s_w_sys)\n",
    "\n",
    "    # -- Metrics --\n",
    "    e_sys_vs_fp32 = (out_sys - ref_fp32).abs()\n",
    "    e_pt_vs_fp32  = (out_pt  - ref_fp32).abs()\n",
    "    e_sys_vs_pt   = (out_sys - out_pt ).abs()\n",
    "\n",
    "    print(f\"FP32 out shape: {ref_fp32.shape}\")\n",
    "    print(f\"Sys vs FP32:  max={e_sys_vs_fp32.max().item():.6f}, mean={e_sys_vs_fp32.mean().item():.6f}\")\n",
    "    print(f\"PT  vs FP32:  max={e_pt_vs_fp32.max().item():.6f}, mean={e_pt_vs_fp32.mean().item():.6f}\")\n",
    "    print(f\"Sys vs PT :   max={e_sys_vs_pt.max().item():.6f},  mean={e_sys_vs_pt.mean().item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Compare Linear: Systolic INT8 vs PyTorch INT8 ===\n",
      "FP32 out shape: torch.Size([4, 5])\n",
      "Sys vs FP32:  max=0.035596, mean=0.013366\n",
      "PT  vs FP32:  max=7.577308, mean=2.424562\n",
      "Sys vs PT :   max=7.593278,  mean=2.427414\n"
     ]
    }
   ],
   "source": [
    "compare_linear_systolic_vs_pytorch_int8(M=4, K=6, N=5)\n",
    "#compare_conv_systolic_vs_pytorch_int8(N=1, C=2, H=7, W=7, O=3, Kh=3, Kw=3, stride=(1,1), padding=(1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
