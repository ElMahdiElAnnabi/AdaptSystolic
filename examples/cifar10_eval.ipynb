{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98a115dd",
   "metadata": {},
   "source": [
    "# Model evaluation and re-training with AdaPT on Cifar10 dataset\n",
    "\n",
    "In this notebook you can evaluate different approximate multipliers on various models based on Cifar10 dataset\n",
    "\n",
    "Steps:\n",
    "* Select models to load \n",
    "* Select number of threads to use\n",
    "* Choose approximate multiplier \n",
    "* Load model for evaluation\n",
    "* Load dataset\n",
    "* Run model calibration for quantization\n",
    "* Run model evaluation\n",
    "* Run approximate-aware re-training\n",
    "* Rerun model evaluation\n",
    "\n",
    "**Note**:\n",
    "* This notebook should be run on a X86 machine\n",
    "\n",
    "* Please make sure you have run the installation steps first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5eef0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import torch\n",
    "\n",
    "import requests\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "from torchvision.datasets import CIFAR10\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab49b35",
   "metadata": {},
   "source": [
    "## Select models to load \n",
    "\n",
    "The weights must be downloaded in state_dicts folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "183a13aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.resnet import resnet18, resnet34, resnet50\n",
    "from models.vgg import vgg11_bn, vgg13_bn, vgg19_bn\n",
    "from models.densenet import densenet121, densenet161, densenet169\n",
    "from models.inception import inception_v3 # slow, propably bad cifar10 implementation of inception for PT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69265983",
   "metadata": {},
   "source": [
    "## Select number of threads to use\n",
    "\n",
    "For optimal performance set them as the number of your cpu threads (not cpu cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "165c2d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OMP_PLACES=cores\n",
      "env: OMP_PROC_BIND=close\n",
      "env: OMP_WAIT_POLICY=active\n"
     ]
    }
   ],
   "source": [
    "threads = 40\n",
    "torch.set_num_threads(threads)\n",
    "\n",
    "#maybe better performance\n",
    "%env OMP_PLACES=cores\n",
    "%env OMP_PROC_BIND=close\n",
    "%env OMP_WAIT_POLICY=active"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa06300",
   "metadata": {},
   "source": [
    "## Choose approximate multiplier \n",
    "\n",
    "Two approximate multipliers are already provided\n",
    "\n",
    "**mul8s_acc** - (header file: mul8s_acc.h)   <--  default\n",
    "\n",
    "**mul8s_1L2H** - (header file: mul8s_1L2H.h)\n",
    "\n",
    "\n",
    "\n",
    "In order to use your custom multiplier you need to use the provided tool (LUT_generator) to easily create the C++ header for your multiplier. Then you just place it inside the adapt/cpu-kernels/axx_mults folder. The name of the axx_mult here must match the name of the header file. The same axx_mult is used in all layers. \n",
    "\n",
    "Tip: If you want explicitly to set for each layer a different axx_mult you must do it from the model definition using the respective AdaPT_Conv2d class of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "562689c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "axx_mult = 'mul8s_acc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b539e7e1",
   "metadata": {},
   "source": [
    "## Load model for evaluation\n",
    "\n",
    "Jit compilation method loads 'on the fly' the C++ extentions of the approximate multipliers. Then the pytorch model is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc26796f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/PyInit_conv2d_mul8s_acc...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/PyInit_conv2d_mul8s_acc/build.ninja...\n",
      "Building extension module PyInit_conv2d_mul8s_acc...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/2] c++ -MMD -MF axx_conv2d.o.d -DTORCH_EXTENSION_NAME=PyInit_conv2d_mul8s_acc -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.8/dist-packages/torch/include/THC -isystem /usr/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -DAXX_MULT=mul8s_acc -march=native -fopenmp -O3 -c /workspace/adapt/adapt/cpu-kernels/axx_conv2d.cpp -o axx_conv2d.o \n",
      "[2/2] c++ axx_conv2d.o -shared -lgomp -L/usr/local/lib/python3.8/dist-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o PyInit_conv2d_mul8s_acc.so\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): AdaPT_Conv2d(\n",
       "    3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "    (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "  )\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): AdaPT_Conv2d(\n",
       "        64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): AdaPT_Conv2d(\n",
       "        64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): AdaPT_Conv2d(\n",
       "        64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): AdaPT_Conv2d(\n",
       "        128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): AdaPT_Conv2d(\n",
       "        128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): AdaPT_Conv2d(\n",
       "        128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): AdaPT_Conv2d(\n",
       "        128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): AdaPT_Conv2d(\n",
       "        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): AdaPT_Conv2d(\n",
       "        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): AdaPT_Conv2d(\n",
       "        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): AdaPT_Conv2d(\n",
       "        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): AdaPT_Conv2d(\n",
       "        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): AdaPT_Conv2d(\n",
       "        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): AdaPT_Conv2d(\n",
       "        512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): AdaPT_Conv2d(\n",
       "        512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): AdaPT_Conv2d(\n",
       "        512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = resnet50(pretrained=True, axx_mult = axx_mult)\n",
    "\n",
    "model.eval() # for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76721ed0",
   "metadata": {},
   "source": [
    "## Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f63b4701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def val_dataloader(mean = (0.4914, 0.4822, 0.4465), std = (0.2471, 0.2435, 0.2616)):\n",
    "\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean, std),\n",
    "        ]\n",
    "    )\n",
    "    dataset = CIFAR10(root=\"datasets/cifar10_data\", train=False, download=True, transform=transform)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=True,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "transform = T.Compose(\n",
    "        [\n",
    "            T.RandomCrop(32, padding=4),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean = (0.4914, 0.4822, 0.4465), std = (0.2471, 0.2435, 0.2616)),\n",
    "        ]\n",
    "    )\n",
    "dataset = CIFAR10(root=\"datasets/cifar10_data\", train=True, download=True, transform=transform)\n",
    "\n",
    "evens = list(range(0, len(dataset), 10))\n",
    "trainset_1 = torch.utils.data.Subset(dataset, evens)\n",
    "\n",
    "data = val_dataloader()\n",
    "\n",
    "# data_t is used for calibration purposes and is a subset of train-set\n",
    "data_t = DataLoader(trainset_1, batch_size=128,\n",
    "                                            shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa74c5d",
   "metadata": {},
   "source": [
    "## Run model calibration for quantization\n",
    "\n",
    "Calibrates the quantization parameters \n",
    "\n",
    "Need to re-run it each time the model changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "946f0d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:20<00:00, 10.10s/it]\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1108 19:14:57.333475 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.333924 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.334270 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.334684 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.335093 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.335536 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.335944 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.336336 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.336743 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.337203 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.337783 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.338386 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.339003 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.339553 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.340044 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.340636 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.341279 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.341954 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.342475 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.342931 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.343379 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.344864 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.346939 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.347684 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.348211 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.348735 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.349259 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.349736 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.350150 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.350589 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.351018 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.351427 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.351823 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.352175 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.352517 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.352871 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.353217 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.353541 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.353859 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.354177 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.354504 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.354843 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.355202 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.355575 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.355964 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.356378 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.356734 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.357072 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.357423 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.357735 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.358058 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.358374 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.358690 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.359011 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.359333 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.359662 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.360034 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.360482 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.360958 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.361413 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.361849 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.362318 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.362730 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.363122 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.363484 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.363924 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.364446 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.366403 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.369020 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.369546 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.369955 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.370426 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.371187 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.371544 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.371879 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.372199 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.372521 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.372848 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.373171 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.373538 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.373881 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.374214 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.374643 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.374969 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.375350 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.375686 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.376119 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.376457 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.376797 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1108 19:14:57.377121 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.377450 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.377769 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.378094 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.378410 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.378733 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.381019 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.381440 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.381809 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.382157 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.382537 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.382917 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.383282 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.383650 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.384003 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.384347 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.384710 140484954826560 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1108 19:14:57.386188 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.386525 140484954826560 tensor_quantizer.py:238] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "W1108 19:14:57.387022 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.387923 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.388558 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.389017 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.389798 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.390492 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.391164 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.391812 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.392435 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.392928 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.394063 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.395023 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.395704 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.397015 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.397700 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.398474 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.399072 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.400247 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.401028 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.401901 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.402729 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.403553 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.404508 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.405426 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.406271 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.407057 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.407774 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.408481 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.409323 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.410146 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.410962 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.411768 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.412554 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.413395 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.414109 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.414806 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.415551 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.416437 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.417373 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.418277 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.419127 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.419954 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.420753 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.421535 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.422336 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.423177 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.424558 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.425443 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.426150 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.426818 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.427472 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.428122 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.428767 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.429486 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.430292 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.431164 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.431848 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.432544 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.433402 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.434241 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1108 19:14:57.434852 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.435732 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.436209 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.437160 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.437812 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.438928 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.439581 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.440821 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.441751 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.442603 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.443441 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.444297 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.445174 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.446094 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.447067 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.447936 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.448775 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.449674 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.450526 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.451373 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.452172 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.453037 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.453953 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.454789 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.455611 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.456498 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.457453 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.458343 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.459243 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.460156 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.461038 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.461907 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.462997 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.464149 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.465245 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.466805 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.467855 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.468694 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.469409 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.470103 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.470779 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.471456 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.471929 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.472763 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1108 19:14:57.473583 140484954826560 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.quantizer                         : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator quant)\n",
      "conv1.quantizer_w                       : TensorQuantizer(8bit per-tensor amax=0.1625 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.6147 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0804 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2123 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0355 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2298 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0544 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.6147 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0867 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2267 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0270 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1345 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0287 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1646 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0338 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2274 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0412 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1737 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0334 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3318 calibrator=HistogramCalibrator quant)\n",
      "layer1.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0479 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2297 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0423 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1703 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0212 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2024 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0298 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.2297 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0273 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1462 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0200 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1391 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0167 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1568 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0267 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1449 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0243 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1425 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0189 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1556 calibrator=HistogramCalibrator quant)\n",
      "layer2.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0303 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1459 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0273 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1448 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0215 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1686 calibrator=HistogramCalibrator quant)\n",
      "layer2.3.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0277 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1581 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0266 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1402 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0128 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1899 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0160 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.1581 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0104 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0984 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0125 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0922 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0107 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0927 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0177 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1077 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0122 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0824 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0860 calibrator=HistogramCalibrator quant)\n",
      "layer3.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0172 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1205 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0111 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0989 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0081 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1284 calibrator=HistogramCalibrator quant)\n",
      "layer3.3.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0151 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1377 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0125 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1070 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0084 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1329 calibrator=HistogramCalibrator quant)\n",
      "layer3.4.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0161 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1927 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0124 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1135 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0068 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1457 calibrator=HistogramCalibrator quant)\n",
      "layer3.5.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0185 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4996 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0093 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.0826 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0029 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2713 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0087 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4996 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0131 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5318 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0029 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1478 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0040 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3085 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0107 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.8086 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0024 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1313 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0042 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv3.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3067 calibrator=HistogramCalibrator quant)\n",
      "layer4.2.conv3.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0111 calibrator=HistogramCalibrator quant)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import calib\n",
    "\n",
    "def collect_stats(model, data_loader, num_batches):\n",
    "     \"\"\"Feed data to the network and collect statistic\"\"\"\n",
    "\n",
    "     # Enable calibrators\n",
    "     for name, module in model.named_modules():\n",
    "         if isinstance(module, quant_nn.TensorQuantizer):\n",
    "             if module._calibrator is not None:\n",
    "                 module.disable_quant()\n",
    "                 module.enable_calib()\n",
    "             else:\n",
    "                 module.disable()\n",
    "\n",
    "     for i, (image, _) in tqdm(enumerate(data_loader), total=num_batches):\n",
    "         model(image.cpu())\n",
    "         if i >= num_batches:\n",
    "             break\n",
    "\n",
    "     # Disable calibrators\n",
    "     for name, module in model.named_modules():\n",
    "         if isinstance(module, quant_nn.TensorQuantizer):\n",
    "             if module._calibrator is not None:\n",
    "                 module.enable_quant()\n",
    "                 module.disable_calib()\n",
    "             else:\n",
    "                 module.enable()\n",
    "\n",
    "def compute_amax(model, **kwargs):\n",
    " # Load calib result\n",
    " for name, module in model.named_modules():\n",
    "     if isinstance(module, quant_nn.TensorQuantizer):\n",
    "         if module._calibrator is not None:\n",
    "             if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "                 module.load_calib_amax()\n",
    "             else:\n",
    "                 module.load_calib_amax(**kwargs)\n",
    "         print(F\"{name:40}: {module}\")\n",
    " model.cpu()\n",
    "\n",
    "# It is a bit slow since we collect histograms on CPU\n",
    "with torch.no_grad():\n",
    "    stats = collect_stats(model, data_t, num_batches=2)\n",
    "    amax = compute_amax(model, method=\"percentile\", percentile=99.99)\n",
    "    \n",
    "    # optional - test different calibration methods\n",
    "    #amax = compute_amax(model, method=\"mse\")\n",
    "    #amax = compute_amax(model, method=\"entropy\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446f0bd",
   "metadata": {},
   "source": [
    "## Run model evaluation\n",
    "\n",
    "Tip: observe how the execution becomes faster and faster with each batch as the CPU achieves better cache re-use on the LUT table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44bfa498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 78/78 [14:48<00:00, 11.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "888.2028056679992\n",
      "Accuracy of the network on the 10000 test images: 93.5597 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "    for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(timeit.default_timer() - start_time)\n",
    "print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9204303c",
   "metadata": {},
   "source": [
    "## Run approximate-aware re-training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "949ec29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [ 0/40]  eta: 0:34:44  lr: 0.0001  img/s: 2.4589313633531336  loss: 0.0322 (0.0322)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 52.1060  data: 0.0507\n",
      "Epoch: [0]  [ 1/40]  eta: 0:33:06  lr: 0.0001  img/s: 2.5806590472004944  loss: 0.0281 (0.0301)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 50.9298  data: 0.1023\n",
      "Epoch: [0]  [ 2/40]  eta: 0:35:03  lr: 0.0001  img/s: 1.9982702357262567  loss: 0.0322 (0.0334)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 55.3482  data: 0.1114\n",
      "Epoch: [0]  [ 3/40]  eta: 0:35:28  lr: 0.0001  img/s: 2.0026249871504636  loss: 0.0322 (0.0454)  acc1: 100.0000 (99.6094)  acc5: 100.0000 (100.0000)  time: 57.5336  data: 0.1269\n",
      "Epoch: [0]  [ 4/40]  eta: 0:35:41  lr: 0.0001  img/s: 1.9060153984495398  loss: 0.0329 (0.0429)  acc1: 100.0000 (99.6875)  acc5: 100.0000 (100.0000)  time: 59.4761  data: 0.1196\n",
      "Epoch: [0]  [ 5/40]  eta: 0:35:44  lr: 0.0001  img/s: 1.8227123836482317  loss: 0.0329 (0.0432)  acc1: 100.0000 (99.6094)  acc5: 100.0000 (100.0000)  time: 61.2850  data: 0.1170\n",
      "Epoch: [0]  [ 6/40]  eta: 0:34:29  lr: 0.0001  img/s: 2.196384865557333  loss: 0.0399 (0.0450)  acc1: 100.0000 (99.5536)  acc5: 100.0000 (100.0000)  time: 60.8761  data: 0.1210\n",
      "Epoch: [0]  [ 7/40]  eta: 0:33:23  lr: 0.0001  img/s: 2.155666246656368  loss: 0.0399 (0.0474)  acc1: 99.2188 (99.3164)  acc5: 100.0000 (100.0000)  time: 60.7032  data: 0.1202\n",
      "Epoch: [0]  [ 8/40]  eta: 0:31:35  lr: 0.0001  img/s: 2.703748398556685  loss: 0.0399 (0.0456)  acc1: 100.0000 (99.3924)  acc5: 100.0000 (100.0000)  time: 59.2301  data: 0.1184\n",
      "Epoch: [0]  [ 9/40]  eta: 0:29:57  lr: 0.0001  img/s: 2.7445022619446107  loss: 0.0338 (0.0444)  acc1: 100.0000 (99.4531)  acc5: 100.0000 (100.0000)  time: 57.9870  data: 0.1226\n",
      "Epoch: [0]  [10/40]  eta: 0:28:38  lr: 0.0001  img/s: 2.5543030746459325  loss: 0.0399 (0.0443)  acc1: 100.0000 (99.4318)  acc5: 100.0000 (100.0000)  time: 57.2781  data: 0.1185\n",
      "Epoch: [0]  [11/40]  eta: 0:28:11  lr: 0.0001  img/s: 1.8390514581460626  loss: 0.0338 (0.0433)  acc1: 100.0000 (99.4792)  acc5: 100.0000 (100.0000)  time: 58.3121  data: 0.1157\n",
      "Epoch: [0]  [12/40]  eta: 0:27:35  lr: 0.0001  img/s: 1.8553457905167383  loss: 0.0338 (0.0424)  acc1: 100.0000 (99.5192)  acc5: 100.0000 (100.0000)  time: 59.1418  data: 0.1152\n",
      "Epoch: [0]  [13/40]  eta: 0:26:44  lr: 0.0001  img/s: 2.0343668227896337  loss: 0.0338 (0.0429)  acc1: 100.0000 (99.5536)  acc5: 100.0000 (100.0000)  time: 59.4179  data: 0.1132\n",
      "Epoch: [0]  [14/40]  eta: 0:25:50  lr: 0.0001  img/s: 2.0535696213617576  loss: 0.0399 (0.0489)  acc1: 100.0000 (99.4271)  acc5: 100.0000 (99.9479)  time: 59.6206  data: 0.1142\n",
      "Epoch: [0]  [15/40]  eta: 0:24:50  lr: 0.0001  img/s: 2.146723544714182  loss: 0.0399 (0.0488)  acc1: 100.0000 (99.3652)  acc5: 100.0000 (99.9512)  time: 59.6280  data: 0.1141\n",
      "Epoch: [0]  [16/40]  eta: 0:23:52  lr: 0.0001  img/s: 2.119873131453091  loss: 0.0440 (0.0507)  acc1: 100.0000 (99.3107)  acc5: 100.0000 (99.9540)  time: 59.6788  data: 0.1139\n",
      "Epoch: [0]  [17/40]  eta: 0:22:50  lr: 0.0001  img/s: 2.198590148630085  loss: 0.0399 (0.0496)  acc1: 99.2188 (99.3056)  acc5: 100.0000 (99.9566)  time: 59.6039  data: 0.1138\n",
      "Epoch: [0]  [18/40]  eta: 0:21:50  lr: 0.0001  img/s: 2.17144039420804  loss: 0.0399 (0.0484)  acc1: 100.0000 (99.3421)  acc5: 100.0000 (99.9589)  time: 59.5766  data: 0.1150\n",
      "Epoch: [0]  [19/40]  eta: 0:20:49  lr: 0.0001  img/s: 2.192399082453528  loss: 0.0380 (0.0479)  acc1: 99.2188 (99.3359)  acc5: 100.0000 (99.9609)  time: 59.5227  data: 0.1151\n",
      "Epoch: [0]  [20/40]  eta: 0:19:49  lr: 0.0001  img/s: 2.193415747833813  loss: 0.0380 (0.0470)  acc1: 99.2188 (99.3676)  acc5: 100.0000 (99.9628)  time: 59.8409  data: 0.1181\n",
      "Epoch: [0]  [21/40]  eta: 0:18:44  lr: 0.0001  img/s: 2.4342281596223985  loss: 0.0399 (0.0472)  acc1: 99.2188 (99.3608)  acc5: 100.0000 (99.9645)  time: 59.9868  data: 0.1149\n",
      "Epoch: [0]  [22/40]  eta: 0:17:38  lr: 0.0001  img/s: 2.487664228444468  loss: 0.0429 (0.0470)  acc1: 99.2188 (99.3886)  acc5: 100.0000 (99.9660)  time: 59.3547  data: 0.1128\n",
      "Epoch: [0]  [23/40]  eta: 0:16:27  lr: 0.0001  img/s: 3.1865174649136456  loss: 0.0380 (0.0466)  acc1: 99.2188 (99.3815)  acc5: 100.0000 (99.9674)  time: 58.1639  data: 0.1094\n",
      "Epoch: [0]  [24/40]  eta: 0:15:20  lr: 0.0001  img/s: 2.890159789491082  loss: 0.0404 (0.0463)  acc1: 99.2188 (99.3750)  acc5: 100.0000 (99.9688)  time: 57.0204  data: 0.1093\n",
      "Epoch: [0]  [25/40]  eta: 0:14:22  lr: 0.0001  img/s: 2.2640638740982393  loss: 0.0398 (0.0461)  acc1: 99.2188 (99.3990)  acc5: 100.0000 (99.9700)  time: 56.3355  data: 0.1089\n",
      "Epoch: [0]  [26/40]  eta: 0:13:24  lr: 0.0001  img/s: 2.240940821051322  loss: 0.0380 (0.0454)  acc1: 99.2188 (99.4213)  acc5: 100.0000 (99.9711)  time: 56.2773  data: 0.1086\n",
      "Epoch: [0]  [27/40]  eta: 0:12:24  lr: 0.0001  img/s: 2.4411319508763443  loss: 0.0380 (0.0457)  acc1: 99.2188 (99.4141)  acc5: 100.0000 (99.9721)  time: 55.9320  data: 0.1105\n",
      "Epoch: [0]  [28/40]  eta: 0:11:29  lr: 0.0001  img/s: 2.0705793014172653  loss: 0.0398 (0.0466)  acc1: 99.2188 (99.3804)  acc5: 100.0000 (99.9731)  time: 56.6558  data: 0.1104\n",
      "Epoch: [0]  [29/40]  eta: 0:10:36  lr: 0.0001  img/s: 1.8085661012825616  loss: 0.0404 (0.0498)  acc1: 99.2188 (99.2708)  acc5: 100.0000 (99.9479)  time: 57.8629  data: 0.1108\n",
      "Epoch: [0]  [30/40]  eta: 0:09:43  lr: 0.0001  img/s: 1.7532259292501275  loss: 0.0404 (0.0511)  acc1: 99.2188 (99.2188)  acc5: 100.0000 (99.9496)  time: 59.0129  data: 0.1159\n",
      "Epoch: [0]  [31/40]  eta: 0:08:49  lr: 0.0001  img/s: 1.776420887952963  loss: 0.0404 (0.0505)  acc1: 99.2188 (99.2432)  acc5: 100.0000 (99.9512)  time: 59.1409  data: 0.1212\n",
      "Epoch: [0]  [32/40]  eta: 0:07:53  lr: 0.0001  img/s: 1.8681301008452058  loss: 0.0404 (0.0498)  acc1: 99.2188 (99.2661)  acc5: 100.0000 (99.9527)  time: 59.1191  data: 0.1230\n",
      "Epoch: [0]  [33/40]  eta: 0:06:57  lr: 0.0001  img/s: 1.7057258661248649  loss: 0.0404 (0.0500)  acc1: 99.2188 (99.2417)  acc5: 100.0000 (99.9540)  time: 59.7278  data: 0.1256\n",
      "Epoch: [0]  [34/40]  eta: 0:05:59  lr: 0.0001  img/s: 1.7609439081523761  loss: 0.0404 (0.0500)  acc1: 99.2188 (99.2634)  acc5: 100.0000 (99.9554)  time: 60.2485  data: 0.1284\n",
      "Epoch: [0]  [35/40]  eta: 0:05:01  lr: 0.0001  img/s: 1.7311409750953963  loss: 0.0398 (0.0495)  acc1: 99.2188 (99.2622)  acc5: 100.0000 (99.9566)  time: 60.9681  data: 0.1323\n",
      "Epoch: [0]  [36/40]  eta: 0:04:02  lr: 0.0001  img/s: 1.7882639163549443  loss: 0.0380 (0.0489)  acc1: 99.2188 (99.2821)  acc5: 100.0000 (99.9578)  time: 61.5301  data: 0.1345\n",
      "Epoch: [0]  [37/40]  eta: 0:03:02  lr: 0.0001  img/s: 2.0838972009089267  loss: 0.0398 (0.0487)  acc1: 99.2188 (99.3010)  acc5: 100.0000 (99.9589)  time: 61.6922  data: 0.1364\n",
      "Epoch: [0]  [38/40]  eta: 0:02:00  lr: 0.0001  img/s: 2.5478882874637656  loss: 0.0398 (0.0481)  acc1: 99.2188 (99.3189)  acc5: 100.0000 (99.9599)  time: 61.2566  data: 0.1362\n",
      "Epoch: [0]  [39/40]  eta: 0:00:59  lr: 0.0001  img/s: 2.0842814318904868  loss: 0.0404 (0.0675)  acc1: 99.2188 (99.2800)  acc5: 100.0000 (99.9600)  time: 58.5246  data: 0.1315\n",
      "Epoch: [0] Total time: 0:39:21\n"
     ]
    }
   ],
   "source": [
    "from adapt.references.classification.train import evaluate, train_one_epoch, load_data\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "# finetune the model for one epoch based on data_t subset \n",
    "train_one_epoch(model, criterion, optimizer, data_t, \"cpu\", 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd0bfa0",
   "metadata": {},
   "source": [
    "## Rerun model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be1ca5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|████████████████████████▊                  | 45/78 [08:57<06:34, 11.95s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteraction, (images, labels) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(data), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(data)):\n\u001b[1;32m      8\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m), labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m/workspace/adapt/examples/models/resnet.py:278\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[1;32m    277\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[0;32m--> 278\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n\u001b[1;32m    280\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py:119\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m/workspace/adapt/examples/models/resnet.py:145\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    142\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[1;32m    143\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m--> 145\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(out)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m/workspace/adapt/adapt/approx_layers/axx_layers.py:232\u001b[0m, in \u001b[0;36mAdaPT_Conv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/adapt/adapt/approx_layers/axx_layers.py:229\u001b[0m, in \u001b[0;36mAdaPT_Conv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_conv_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, weight: Tensor, bias: Optional[Tensor]):       \n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAdaPT_Conv2d_Function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantizer_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantizer_w\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxx_conv2d_kernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/adapt/adapt/approx_layers/axx_layers.py:119\u001b[0m, in \u001b[0;36mAdaPT_Conv2d_Function.forward\u001b[0;34m(ctx, input, weight, quantizer, quantizer_w, kernel_size, amax, amax_w, max_value, out_channels, bias_, axx_conv2d_kernel, bias, stride, padding, dilation, groups, padding_mode)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39mpadding_mode),\n\u001b[1;32m    116\u001b[0m                     weight, bias, stride,\n\u001b[1;32m    117\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), dilation, groups)\n\u001b[0;32m--> 119\u001b[0m quant_weight \u001b[38;5;241m=\u001b[39m \u001b[43mquantizer_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m quant_input \u001b[38;5;241m=\u001b[39m quantizer(\u001b[38;5;28minput\u001b[39m)    \n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m#in case of normal padding_mode run approx_conv2d\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m#TODO quant case for bias=True. Currently not needed for typical ConvNets\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m#quant descriptors inside class is slower than using quant_nn.Conv2d instead\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m/etc/pytorch-quantization/pytorch_quantization/nn/modules/tensor_quantizer.py:345\u001b[0m, in \u001b[0;36mTensorQuantizer.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip(inputs)\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_if_quant:\n\u001b[0;32m--> 345\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_quant_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/etc/pytorch-quantization/pytorch_quantization/nn/modules/tensor_quantizer.py:315\u001b[0m, in \u001b[0;36mTensorQuantizer._quant_forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    313\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fb_fake_quant(inputs, amax)\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 315\u001b[0m     outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale \u001b[38;5;241m=\u001b[39m \u001b[43mtensor_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_bits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unsigned\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/etc/pytorch-quantization/pytorch_quantization/tensor_quant.py:271\u001b[0m, in \u001b[0;36mTensorQuantFunction.forward\u001b[0;34m(ctx, inputs, amax, num_bits, unsigned, narrow_range)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03mFollow tensorflow convention, max value is passed in and used to decide scale, instead of inputing scale\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03m    ValueError:\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    270\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(inputs, amax)\n\u001b[0;32m--> 271\u001b[0m outputs, scale \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munsigned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnarrow_range\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# Check if scale overflows FP16\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mhalf \u001b[38;5;129;01mand\u001b[39;00m scale\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m65504\u001b[39m:\n",
      "File \u001b[0;32m/etc/pytorch-quantization/pytorch_quantization/tensor_quant.py:353\u001b[0m, in \u001b[0;36m_tensor_quant\u001b[0;34m(inputs, amax, num_bits, unsigned, narrow_range)\u001b[0m\n\u001b[1;32m    350\u001b[0m     zero_amax_mask \u001b[38;5;241m=\u001b[39m (amax \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m epsilon)\n\u001b[1;32m    351\u001b[0m     scale[zero_amax_mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Value quantized with amax=0 should all be 0\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclamp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mround_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_bound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bound\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m min_amax \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m epsilon:\n\u001b[1;32m    356\u001b[0m     scale[zero_amax_mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m  \u001b[38;5;66;03m# Return 1 makes more sense for values quantized to 0 with amax=0\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "    for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(timeit.default_timer() - start_time)\n",
    "print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
